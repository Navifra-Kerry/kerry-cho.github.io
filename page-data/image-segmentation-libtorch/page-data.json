{"componentChunkName":"component---src-templates-post-js","path":"/image-segmentation-libtorch/","result":{"data":{"post":{"id":"f62d8d44-100c-54f9-9904-98cc9554fc11","title":"Image Segmentation Libtorch","slug":"/image-segmentation-libtorch/","link":null,"excerpt":"안녕하세요. 조대희 입니다.\n블로그 방문을 환영 합니다. 이번에 소개 해드릴 내용은 PyTorch의 C++ Frontend 인 LibTorch를 활용한 Image…","timeToRead":2,"featured":null,"thumbnailText":null,"date":"November 27, 2019","category":{"id":"9551985a-0ac8-5537-8160-577b93e7bd47","name":"deeplearing","slug":"/category/deeplearing/","color":null,"icon":"/static/9db8414257e39f04182ba00bd97007bb/deeplearing.svg"},"author":{"id":"6ade7dba-bac0-5d88-95e2-844c9469c477","name":"Kerry Cho","slug":"/author/kerry-cho/","title":"Software Engineer","description":"","skills":["C++","C","Node"],"social":[{"name":"Instagram","url":"https://instagram.com/instagram"},{"name":"Twitter","url":"https://twitter.com/twitter"},{"name":"Website","url":"https://example.com"}],"thumbnail":{"__typename":"ImageSharp","ImageSharp_small":{"layout":"fixed","placeholder":{"fallback":"data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='150'%20height='150'%20viewBox='0%200%20150%20150'%20preserveAspectRatio='none'%3e%3cpath%20d='M60%206c-6%204-10%209-11%2018-2%207-1%209%204%2024l3%209c0%201-1%202-3%202l-5%204c-3%205-7%208-10%207l-2%202c0%203-2%204-2%201-1-2-1%205-1%2010l-1%203-1-1c0-2-3-5-4-4l1%207c1%205%201%206-1%2010l-2%205-1%202c-2%202-2%208%200%209%201%201%200%205-1%205s-2%201-2%203l1%203v4c-2%201-1%202%203%201h5c3%201%202%208-2%209l-4%202-2%202%203%201%204%204c2%202%203%202%2043%202h40l2-3c2-2%202-4%202-6v-4l2-4c1-3%201-3-3-3-5-1-6-1-4%201%203%203%201%203-4%200l-5-4v-1l-6-1-5%201%203-2%204-2h-6c-2%201-3%202%200-5%202-5%201-5-2%200-2%203-4%203-4%201l1-1%201-1v-3l-2-2c-1-2-1-2-4-1-2%202-2%202-3%200l-2-2c-1%200-2-3-1-4l1%201h2c0-1%202-1%204%201l4%202c1%200-2-5-4-6-1-2-1-2%203-6%203-4%204-5%203-10a2391%202391%200%2001-1-8h-2v-9l1-8-3-3c-4-3-4-7-1-10s7-23%206-29C87%208%2071%201%2060%206m-6%2062l-4%201c-3%200-6%205-6%208v3c-1%201%200%201%202%201l4%201h4c5-2%2012%200%2011%204-1%203-9%204-9%201l4-1c3%200%203%200%202-1H44c-1%201%200%201%201%201%204%200%204%201%202%203h-4c-2-2-3%200-2%204%201%205%203%206%2011%206s12-1%2014-6c2-3%203-9%201-9l-2-5c0-4-1-5-4-8-5-4-5-4-7-3m50%2012l-4%208-5%2010c-1%203-3%207-2%208l-2%202c-2%201-2%201%201%202%202%201%204%202%206%207%203%207%206%209%2010%209l2%201c2%202%206%202%208-1%203-4%203-4-1-3l-4-1-7-2c-5%200-5-1-5-3-1-1%200-2%204-3l7-6%204-5c1-1%203-14%201-13l-17%208c-1%200%200-3%204-6%203-5%204-8%201-4-4%203-4%202%200-5%203-6%203-8-1-3'%20fill='%23d3d3d3'%20fill-rule='evenodd'/%3e%3c/svg%3e"},"images":{"fallback":{"src":"/static/852c30a1756768fa576889f312b2bd64/a7a66/kerry-cho.png","srcSet":"/static/852c30a1756768fa576889f312b2bd64/a7a66/kerry-cho.png 48w,\n/static/852c30a1756768fa576889f312b2bd64/416a0/kerry-cho.png 96w","sizes":"48px"},"sources":[{"srcSet":"/static/852c30a1756768fa576889f312b2bd64/a3542/kerry-cho.webp 48w,\n/static/852c30a1756768fa576889f312b2bd64/0f66d/kerry-cho.webp 96w","type":"image/webp","sizes":"48px"}]},"width":48,"height":48},"ImageSharp_regular":{"layout":"fixed","placeholder":{"fallback":"data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='150'%20height='150'%20viewBox='0%200%20150%20150'%20preserveAspectRatio='none'%3e%3cpath%20d='M60%206c-6%204-10%209-11%2018-2%207-1%209%204%2024l3%209c0%201-1%202-3%202l-5%204c-3%205-7%208-10%207l-2%202c0%203-2%204-2%201-1-2-1%205-1%2010l-1%203-1-1c0-2-3-5-4-4l1%207c1%205%201%206-1%2010l-2%205-1%202c-2%202-2%208%200%209%201%201%200%205-1%205s-2%201-2%203l1%203v4c-2%201-1%202%203%201h5c3%201%202%208-2%209l-4%202-2%202%203%201%204%204c2%202%203%202%2043%202h40l2-3c2-2%202-4%202-6v-4l2-4c1-3%201-3-3-3-5-1-6-1-4%201%203%203%201%203-4%200l-5-4v-1l-6-1-5%201%203-2%204-2h-6c-2%201-3%202%200-5%202-5%201-5-2%200-2%203-4%203-4%201l1-1%201-1v-3l-2-2c-1-2-1-2-4-1-2%202-2%202-3%200l-2-2c-1%200-2-3-1-4l1%201h2c0-1%202-1%204%201l4%202c1%200-2-5-4-6-1-2-1-2%203-6%203-4%204-5%203-10a2391%202391%200%2001-1-8h-2v-9l1-8-3-3c-4-3-4-7-1-10s7-23%206-29C87%208%2071%201%2060%206m-6%2062l-4%201c-3%200-6%205-6%208v3c-1%201%200%201%202%201l4%201h4c5-2%2012%200%2011%204-1%203-9%204-9%201l4-1c3%200%203%200%202-1H44c-1%201%200%201%201%201%204%200%204%201%202%203h-4c-2-2-3%200-2%204%201%205%203%206%2011%206s12-1%2014-6c2-3%203-9%201-9l-2-5c0-4-1-5-4-8-5-4-5-4-7-3m50%2012l-4%208-5%2010c-1%203-3%207-2%208l-2%202c-2%201-2%201%201%202%202%201%204%202%206%207%203%207%206%209%2010%209l2%201c2%202%206%202%208-1%203-4%203-4-1-3l-4-1-7-2c-5%200-5-1-5-3-1-1%200-2%204-3l7-6%204-5c1-1%203-14%201-13l-17%208c-1%200%200-3%204-6%203-5%204-8%201-4-4%203-4%202%200-5%203-6%203-8-1-3'%20fill='%23d3d3d3'%20fill-rule='evenodd'/%3e%3c/svg%3e"},"images":{"fallback":{"src":"/static/852c30a1756768fa576889f312b2bd64/46e60/kerry-cho.png","srcSet":"/static/852c30a1756768fa576889f312b2bd64/46e60/kerry-cho.png 150w,\n/static/852c30a1756768fa576889f312b2bd64/d612b/kerry-cho.png 300w","sizes":"150px"},"sources":[{"srcSet":"/static/852c30a1756768fa576889f312b2bd64/ae23d/kerry-cho.webp 150w,\n/static/852c30a1756768fa576889f312b2bd64/bd37b/kerry-cho.webp 300w","type":"image/webp","sizes":"150px"}]},"width":150,"height":150}}},"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Image Segmentation Libtorch\",\n  \"category\": \"deeplearing\",\n  \"author\": \"Kerry Cho\",\n  \"tags\": [\"#Libtorch\", \"#Pytorh\"],\n  \"date\": \"2019-11-27T00:00:00.000Z\",\n  \"thumbnail\": \"image.jpg\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"\\uC548\\uB155\\uD558\\uC138\\uC694. \\uC870\\uB300\\uD76C \\uC785\\uB2C8\\uB2E4.\\n\\uBE14\\uB85C\\uADF8 \\uBC29\\uBB38\\uC744 \\uD658\\uC601 \\uD569\\uB2C8\\uB2E4.\"), mdx(\"p\", null, \"\\uC774\\uBC88\\uC5D0 \\uC18C\\uAC1C \\uD574\\uB4DC\\uB9B4 \\uB0B4\\uC6A9\\uC740 PyTorch\\uC758 C++ Frontend \\uC778 LibTorch\\uB97C \\uD65C\\uC6A9\\uD55C Image Segmentation\\uC744 \\uD558\\uB294 \\uBC95\\uC5D0 \\uB300\\uD55C \\uB0B4\\uC6A9 \\uC785\\uB2C8\\uB2E4.\\n\\uBAA8\\uB4E0 \\uC18C\\uC2A4\\uB294 \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/kerry-Cho/SemanticSegmentation-Libtorch\"\n  }, \"\\uC5EC\\uAE30\"), \" \\uC800\\uC7A5\\uC18C\\uC5D0 \\uC788\\uC2B5\\uB2C8\\uB2E4. \"), mdx(\"p\", null, \"\\uBCF8 \\uD3EC\\uC2A4\\uD2B8\\uB97C \\uC77D\\uAE30 \\uC804 \\uC0AC\\uC804\\uC5D0 \\uACE0\\uC804\\uC801\\uC778 Image Segmentation\\uACFC Image Processing\\uC5D0 \\uB300\\uD55C \\uAE30\\uCD08 \\uC9C0\\uC2DD\\uC774 \\uD544\\uC694 \\uD569\\uB2C8\\uB2E4.\\n\\uD574\\uB2F9 \\uB0B4\\uC6A9\\uC744 \\uC81C\\uAC00 \\uC124\\uBA85 \\uD574\\uB4DC\\uB9AC\\uB294 \\uAC83 \\uBCF4\\uB2E4, \\uC804\\uBB38\\uC801\\uC778 \\uAC15\\uC758\\uB97C \\uCD94\\uCC9C \\uB4DC\\uB9BD\\uB2C8\\uB2E4. \"), mdx(\"p\", null, \"Microsoft\\uC5D0\\uC11C\\uB294 \\uBB34\\uB8CC \\uAC15\\uC758\\uC640 \\uB3C4\\uD050\\uBA3C\\uD2B8\\uB97C \\uC798\\uB9CC\\uB4E4\\uAE30\\uB85C \\uC720\\uBA85 \\uD55C\\uB370\\uC694 (\\uC800\\uB9CC \\uADF8\\uB807\\uAC8C \\uC0DD\\uAC01\\uD558\\uB098\\uC694?) \\uC815\\uB9D0 \\uC774\\uB807\\uAC8C \\uD574\\uB3C4 \\uB418\\uB098 \\uC2F6\\uC740\\uB370\\nPyTorch \\uD3EC\\uC2A4\\uD305\\uC744 \\uD558\\uBA74\\uC11C Microsoft\\uC5D0\\uC11C \\uB9CC\\uB4E0 \\uBB34\\uB8CC \\uAD50\\uC721\\uC744 \\uCD94\\uCC9C \\uB4DC\\uB9BD\\uB2C8\\uB2E4. 1\\uB2EC\\uAC04 \\uBB34\\uB8CC\\uB85C \\uC2DC\\uCCAD \\uD558\\uC2E4 \\uC218 \\uC788\\uB294 \\uAC83\\uC73C\\uB85C \\uC54C\\uACE0 \\uC788\\uACE0,\\n\\uD574\\uB2F9 \\uC0AC\\uC774\\uD2B8\\uC758 \\uACBD\\uC6B0 \\uC778\\uC99D\\uC11C \\uAC19\\uC740 \\uAC83\\uC744 \\uBC1C\\uAE09\\uD574 \\uC8FC\\uB294\\uB370 \\uADF8\\uAC83\\uC744 \\uBC1B\\uAE30 \\uC704\\uD574\\uC11C \\uC77C\\uC815\\uC758 \\uB3C8\\uC744 \\uC9C0\\uAE09\\uD574\\uC57C \\uD558\\uB294 \\uAC83\\uC73C\\uB85C \\uC54C\\uACE0 \\uC788\\uC2B5\\uB2C8\\uB2E4. \"), mdx(\"p\", null, \"\\uC9C0\\uAE08 \\uCD94\\uCC9C\\uB4DC\\uB9AC\\uB294 \\uAC15\\uC758\\uBCF4\\uB2E4 \\uC88B\\uC740 \\uAC83\\uB4E4\\uC774 \\uB9CE\\uC774 \\uC788\\uC73C\\uB2C8 \\uACF5\\uBD80 \\uD558\\uC2E4\\uB54C \\uCC38\\uACE0 \\uD558\\uBA74 \\uC88B\\uC744\\uAC83 \\uAC19\\uC2B5\\uB2C8\\uB2E4.. \\uC0AC\\uC774\\uD2B8 \\uC8FC\\uC18C\\uB294 \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://courses.edx.org/courses/course-v1:Microsoft+DEV290x+3T2019/course/\"\n  }, \"\\uC5EC\\uAE30\"), \" \\uC785\\uB2C8\\uB2E4. \\uC544 \\uADF8\\uB9AC\\uACE0 \\uC0AC\\uC774\\uD2B8\\uC758 \\uAC15\\uC758\\uB294 \\uC601\\uC5B4\\uB85C \\uC9C4\\uD589 \\uB429\\uB2C8\\uB2E4. \\uC601\\uC5B4 \\uC6B8\\uB801\\uC99D\\uC774 \\uC788\\uC73C\\uC2E0 \\uBD84\\uB4E4\\uC5D0\\uAC8C\\uB294 \\uC8C4\\uC1A1\\uD569\\uB2C8\\uB2E4\\u2026\"), mdx(\"p\", null, \"\\uADF8\\uB7FC \\uC2DC\\uC791\\uD558\\uACA0\\uC2B5\\uB2C8\\uB2E4.\"), mdx(\"h1\", {\n    \"id\": \"introdution\"\n  }, \"Introdution\"), mdx(\"p\", null, \"\\uB525\\uB7EC\\uB2DD\\uC744 \\uC774\\uC6A9\\uD55C Python \\uC608\\uC81C\\uB294 \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/mrgloom/awesome-semantic-segmentation\"\n  }, \"\\uC5EC\\uAE30\"), \"\\uC5D0\\uC11C \\uCC3E\\uC544 \\uBCF4\\uC2E4 \\uC218 \\uC788\\uC2B5\\uB2C8\\uB2E4.\\n\\uD574\\uB2F9 \\uC0AC\\uC774\\uD2B8\\uC5D0 \\uB9CE\\uC740 \\uC608\\uC81C\\uB4E4\\uC774 \\uC788\\uC9C0\\uB9CC \\uC800\\uB294 PyTorch\\uB97C \\uC0AC\\uC6A9 \\uD588\\uAE30 \\uB54C\\uBB38\\uC5D0 TorchVision Reference\\uB97C \\uCC38\\uACE0 \\uD558\\uC5EC LibTorch \\uB9CC\\uB4E4\\uC5B4 \\uBCF4\\uC558\\uC2B5\\uB2C8\\uB2E4.\"), mdx(\"p\", null, \"\\uADF8\\uB9AC\\uACE0 \\uB370\\uC774\\uD130 \\uC14B\\uC758 \\uC18C\\uC2A4\\uB294 \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/lsrock1/maskrcnn_benchmark.cpp\"\n  }, \"\\uC774\\uCE5C\\uAD6C\"), \"\\uC758 \\uC18C\\uC2A4\\uB97C \\uCC38\\uACE0\\uD574\\uC11C \\uC81C\\uAC00 \\uC0AC\\uC6A9\\uD558\\uAE30 \\uD3B8\\uD55C \\uBC29\\uBC95\\uC73C\\uB85C \\uBCC0\\uACBD \\uD558\\uC600\\uC2B5\\uB2C8\\uB2E4.\\n\\uC704\\uC758 \\uCE5C\\uAD6C\\uAC00 \\uC9C0\\uAE08 \\uC5F4\\uC2EC\\uD788 MaskRCNN LibTorch \\uBC84\\uC804\\uC744 \\uB9CC\\uB4E4\\uACE0 \\uC788\\uB358\\uB370 \\uB2E4 \\uB9CC\\uB4E4\\uC5B4 \\uC9C0\\uBA74 \\uC800\\uAC83 \\uB610\\uD55C \\uB9AC\\uBDF0\\uB97C \\uD558\\uB294 \\uC2DC\\uAC04\\uC744 \\uAC00\\uC838 \\uBCFC\\uAED8\\uC694\\u2026\\uC544\\uB2C8\\uBA74 \\uC81C\\uAC00 \\uB9CC\\uB4E4\\uC5B4\\uC11C!!?\"), mdx(\"h1\", {\n    \"id\": \"install-dependency\"\n  }, \"Install Dependency\"), mdx(\"p\", null, \"\\uC758\\uC874\\uC131 \\uD30C\\uC77C\\uB4E4\\uC740 \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://kerry-cho.github.io/TransferLearning-Libtorch/\"\n  }, \"TransferLearning\"), \" \\uD3EC\\uC2A4\\uD2B8\\uB97C \\uCC38\\uACE0 \\uD558\\uC2DC\\uBA74 \\uB429\\uB2C8\\uB2E4.\"), mdx(\"h1\", {\n    \"id\": \"network-model\"\n  }, \"Network Model\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Backbone\", mdx(\"br\", {\n    parentName: \"li\"\n  }), \"Backbone \\uAC19\\uC740 \\uACBD\\uC6B0 TorchVision\\uC758 Resnet \\uBAA8\\uB378 \\uAC00\\uC838\\uC640\\uC11C \\uC870\\uAE08 \\uC218\\uC815 \\uD558\\uC600\\uC2B5\\uB2C8\\uB2E4. Segmentation\\uC758 \\uACBD\\uC6B0 Dilate Convolution\\uC744 \\uC9C4\\uD589 \\uD574\\uC57C \\uD558\\uACE0\\n\\uBAA8\\uB4C8 \\uC804\\uCCB4\\uB97C \\uAC00\\uC838\\uC640 \\uD558\\uB098\\uC758 \\uB124\\uD2B8\\uC6CD\\uC73C\\uB85C \\uAD6C\\uC131 \\uD574\\uC57C \\uD558\\uAE30 \\uB54C\\uBB38\\uC5D0 \\uC218\\uC815\\uC744 \\uC9C4\\uD589 \\uD588\\uC2B5\\uB2C8\\uB2E4.\\n\\uBB3C\\uB860 Python \\uBAA8\\uB378\\uACFC \\uD638\\uD658\\uC774 \\uB418\\uAC8C\\uB054 \\uC218\\uC815 \\uD588\\uC2B5\\uB2C8\\uB2E4. \")), mdx(\"p\", null, \"\\uC911\\uC694\\uD55C \\uB0B4\\uC6A9\\uC740 \\uC544\\uB798\\uC640 \\uAC19\\uC2B5\\uB2C8\\uB2E4.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-c++\"\n  }, \"template <typename Block>\\nstruct ResNetImpl : torch::nn::Module\\n {\\n    ...\\n    int64_t groups, base_width, inplanes;\\n    int64_t _dilation;\\n    torch::nn::Conv2d conv1;\\n    torch::nn::BatchNorm bn1;\\n    torch::nn::Functional max_pool1; \\n    /*\\n        BackBone\\uC73C\\uB85C \\uC0AC\\uC6A9 \\uD558\\uAE30 \\uC704\\uD574\\uC11C\\uB294 max_pool\\uC744 API\\uB85C \\uD638\\uCD9C \\uD558\\uBA74\\n        Segmntation \\uBAA8\\uB378\\uC5D0\\uC11C \\uC0AC\\uC6A9 \\uD560 \\uC218 \\uC5C6\\uAE30 \\uB54C\\uBB38\\uC5D0 Functional \\uBA64\\uBC84 \\uBCC0\\uC218\\uB85C \\uC120\\uC5B8 \\uD574\\uC90D\\uB2C8\\uB2E4.\\n    */\\n    torch::nn::Linear fc;\\n    torch::nn::Sequential layer1, layer2, layer3, layer4;\\n    ...\\n}\\n\\ntemplate <typename Block>\\ntorch::Tensor ResNetImpl<Block>::forward(torch::Tensor x) {\\n    x = conv1->forward(x);\\n    x = bn1->forward(x).relu_();\\n    /*\\n    \\uAE30\\uC874 \\uCF54\\uB4DC \\n    x = torch::max_pool2d(x, 3, 2, 1);\\n    */\\n    x = max_pool1->forward(x);\\n\\n    x = layer1->forward(x);\\n    x = layer2->forward(x);\\n    x = layer3->forward(x);\\n    x = layer4->forward(x);\\n\\n    x = torch::adaptive_avg_pool2d(x, { 1, 1 }); //\\uC774 \\uC544\\uC774\\uB294 \\uC0AC\\uC6A9 \\uD558\\uC9C0 \\uC54A\\uAE30 \\uB54C\\uBB38\\uC5D0 \\uBA64\\uBC84 \\uD568\\uC218\\uB85C \\uC120\\uC5B8 \\uD558\\uC9C0 \\uC54A\\uC558\\uC5B4\\uC694...\\n    x = x.view({ x.sizes()[0], -1 });\\n    x = fc->forward(x);\\n\\n    return x;\\n}\\n\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"IntermediateLayerGetter\", mdx(\"br\", {\n    parentName: \"li\"\n  }), \"Python PyTorch\\uC5D0\\uC11C\\uB294 IntermediateLayer\\uC758 \\uACBD\\uC6B0 \\uBAA8\\uB4E0 Network\\uC744 \\uC785\\uB825 \\uBC1B\\uC544 \\uC6D0\\uD558\\uB294 \\uB808\\uC774\\uC5B4\\uC758 \\uCD9C\\uB825\\uB9CC List\\uC5D0 \\uB2F4\\uC544\\uC11C \\uB9AC\\uD134\\uC744 \\uD574\\uC8FC\\uB294 \\uD074\\uB798\\uC2A4\\uC778\\uB370\\uC694.\\n\\uC800 \\uAC19\\uC740 \\uACBD\\uC6B0\\uC5D0 \\uD604\\uC7AC\\uB294 resnet\\uB9CC \\uC785\\uB825\\uC744 \\uBC1B\\uC544\\uC11C \\uCD9C\\uB825\\uC744 \\uBC1B\\uB294 \\uD615\\uD0DC\\uB85C \\uB418\\uC5B4 \\uC788\\uC2B5\\uB2C8\\uB2E4.\\n\\uC5EC\\uAE30 \\uC800\\uAE30 \\uBB3C\\uC5B4 \\uBD24\\uC9C0\\uB9CC anyModule \\uC0AC\\uC6A9\\uD574\\uC11C \\uD558\\uBA74 \\uB41C\\uB2E4\\uACE0 \\uD558\\uB294\\uB370 \\uADF8\\uB807\\uAC8C \\uD560 \\uACBD\\uC6B0\\uC5D0 weight\\uAC12\\uC744 \\uBCF5\\uC0AC\\uD558\\uAE30\\uAC00 \\uBD88\\uD3B8\\uD574 \\uC608\\uC81C\\uC758 \\uCF54\\uB4DC \\uCC98\\uB7FC \\uAD6C\\uD604 \\uD588\\uC2B5\\uB2C8\\uB2E4.\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-c++\"\n  }, \"\\n/*\\n\\uC0DD\\uC131\\uC790\\uC5D0\\uC11C Resnet\\uC744 \\uC785\\uB825 \\uBC1B\\uC544\\uC11C \\uBAA8\\uB4C8\\uC744 OrderedDict \\uB2F4\\uC544 \\uB193\\uACE0 \\uD574\\uB2F9 \\uBAA8\\uB4C8\\uC740 child \\uBAA8\\uB378\\uB85C \\uB4F1\\uB85D \\uD569\\uB2C8\\uB2E4.\\n\\uC0AC\\uC6A9 \\uBC29\\uBC95\\uC740 \\uC544\\uB798\\uC640 \\uAC19\\uC544\\uC694.\\n\\nResNet101 Resnet;   \\ntorch::load(Resnet, \\\"resnet101_Python.pt\\\");\\n\\n_backbone = IntermediateLayerGetter(IntermediateLayerGetterImpl(std::move(Resnet), {\\\"layer3\\\",\\\"layer4\\\"}));\\n*/\\nclass IntermediateLayerGetterImpl : public torch::nn::Module\\n{\\npublic:\\n\\n    template<typename Net>\\n    IntermediateLayerGetterImpl(Net  Module, std::vector<std::string> return_layers)\\n    {\\n        for (auto children : Module->named_children())\\n        {\\n            if (children.key() == \\\"fc\\\") continue;\\n        \\n            _module.insert(children.key(), std::move(children.value()));\\n            register_module(children.key(), _module[children.key()]);\\n        }\\n\\n        _return_layers.swap(return_layers);\\n    }\\n\\n    ~IntermediateLayerGetterImpl();\\n\\n    std::vector<torch::Tensor>  forward(torch::Tensor x);\\n\\nprivate:\\n    torch::OrderedDict<std::string, std::shared_ptr<Module>> _module;\\n    std::vector<std::string> _return_layers;\\n};\\n\\n/*\\n\\uC544\\uB798\\uC758 forward \\uB54C\\uBB38\\uC5D0 \\uBAA8\\uB4E0 \\uB124\\uD2B8\\uC6CD\\uC744 \\uAD6C\\uD604 \\uD560\\uC218\\uAC00 \\uC5C6\\uC5C8\\uC5B4\\uC694. \\uC2DC\\uAC04\\uC774 \\uC5C6\\uC5C8\\uC2B5\\uB2C8\\uB2E4?\\ntorch::nn::Module\\uC744 as \\uD568\\uC218\\uB97C \\uD1B5\\uD574\\uC11C Typecasting \\uC9C0\\uC6D0 \\uD558\\uB294\\uB370\\uC694. \\uB118\\uACA8 \\uBC1B\\uC740 \\uBAA8\\uB378\\uC758 Type\\uC744 \\uC54C \\uC218\\uAC00 \\uC5C6\\uC5B4\\uC11C \\uD55C\\uB540 \\uD55C\\uB540 \\uBCC0\\uD658 \\uD574\\uC92C\\uC2B5\\uB2C8\\uB2E4.\\n\\uC88B\\uC740 \\uBC29\\uBC95\\uC774 \\uC788\\uB2E4\\uBA74 \\uB313\\uAE00\\uB85C \\uB2EC\\uC544 \\uC8FC\\uC2DC\\uBA74 \\uAC10\\uC0AC \\uD558\\uACA0\\uC2B5\\uB2C8\\uB2E4.\\n\\uC774\\uB807\\uAC8C \\uC0AC\\uC6A9\\uD574\\uB3C4 \\uC131\\uB2A5\\uC5D0\\uB294 \\uC774\\uC0C1\\uC774 \\uC5C6\\uC2B5\\uB2C8\\uB2E4.\\n\\uBCF4\\uAE30\\uAC00 \\uC544\\uB984 \\uB2F5\\uC9C0 \\uC54A\\uC744 \\uBFD0\\uC774\\uC8E0....\\n*/\\nstd::vector<torch::Tensor> IntermediateLayerGetterImpl::forward(torch::Tensor x)\\n{\\n    std::vector<torch::Tensor> results;\\n\\n    x = _module[\\\"conv1\\\"]->as<torch::nn::Conv2d>()->forward(x);  \\n    x = _module[\\\"bn1\\\"]->as<torch::nn::BatchNorm>()->forward(x).relu_();\\n    x = _module[\\\"max_pool1\\\"]->as<torch::nn::Functional>()->forward(x);\\n\\n    x = _module[\\\"layer1\\\"]->as<torch::nn::Sequential>()->forward(x);\\n    x = _module[\\\"layer2\\\"]->as<torch::nn::Sequential>()->forward(x);\\n    x = _module[\\\"layer3\\\"]->as<torch::nn::Sequential>()->forward(x);\\n    results.push_back(x);\\n    x = _module[\\\"layer4\\\"]->as<torch::nn::Sequential>()->forward(x);\\n    results.push_back(x);\\n    \\n    return results;\\n}\\n\\n\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"SegmentationModel\", mdx(\"br\", {\n    parentName: \"li\"\n  }), \"SegmentationModel\\uC740 fcn\\uACFC deeplabv3 \\uBC84\\uC804\\uB9CC \\uAD6C\\uD604\\uC744 \\uD558\\uC600\\uC2B5\\uB2C8\\uB2E4. \\uC774 \\uC608\\uC81C\\uB97C \\uBCF4\\uACE0 \\uB09C\\uD6C4\\uC5D0 \\uC5EC\\uB7EC\\uBD84\\uC740 \\uB2E4\\uB978 \\uBAA8\\uB378\\uB3C4 \\uB9CC\\uB4E4 \\uC790\\uC2E0\\uAC10\\uC774 \\uC0DD\\uAE30 \\uC2E4 \\uAC81\\uB2C8\\uB2E4.\\n\\uC5EC\\uB7EC\\uBD84\\uC774 \\uC9C1\\uC811 \\uAD6C\\uD604 \\uD560 \\uC218 \\uC788\\uAC8C \\uB098\\uBA38\\uC9C0\\uB294 \\uB0A8\\uACA8 \\uB193\\uC558\\uC2B5\\uB2C8\\uB2E4. PullRequest\\uB97C \\uBCF4\\uB0B4 \\uC8FC\\uC2DC\\uBA74 \\uAC10\\uC0AC \\uD569\\uB2C8\\uB2E4.\\n\\uBA64\\uBC84 \\uD568\\uC218\\uB85C \\uBAA8\\uB378\\uC744 \\uC120\\uD0DD \\uD560 \\uC218 \\uC788\\uAC8C \\uAD6C\\uD604 \\uD558\\uC600\\uC2B5\\uB2C8\\uB2E4.  \")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-c++\"\n  }, \"\\n/*\\n    \\uC0AC\\uC6A9 \\uBC95\\uC740 \\uC544\\uB798\\uC640 \\uAC19\\uC544\\uC694.\\n    SegmentationModel segnet;\\n    segnet->deeplabv3_resnet101(false, class_num);\\n    segnet->to(device);\\n*/\\n\\nclass SegmentationModelImpl :public torch::nn::Module\\n{\\npublic:\\n    SegmentationModelImpl();\\n    ~SegmentationModelImpl();\\n\\npublic:\\n    void fcn_resnet101(bool pretrained = false, int64_t num_classes = 21, bool aux = true);\\n    void fcn_resnet50(bool pretrained = false, int64_t num_classes = 21, bool aux = true);\\n\\n    void deeplabv3_resnet101(bool pretrained = false, int64_t num_classes = 21, bool aux = true);\\n    void deeplabv3_resnet50(bool pretrained = false, int64_t num_classes = 21, bool aux = true);\\n\\n    std::unordered_map<std::string, torch::Tensor> forward(torch::Tensor x);\\n    IntermediateLayerGetter _backbone{ nullptr };\\n\\n\\n    torch::nn::Sequential _classifier{ nullptr };\\n    torch::nn::Sequential _aux_classifier{ nullptr };\\n    torch::nn::Sequential _make_FCNHead(int64_t in_channels, int64_t channels);\\n    bool _aux;\\n};\\n\\n/*\\nDeeplabV3 3\\uAE30\\uC900\\uC73C\\uB85C \\uC124\\uBA85\\uC744 \\uB4DC\\uB9B4\\uAED8\\uC694.\\n*/\\nvoid SegmentationModelImpl::deeplabv3_resnet101(bool pretrained, int64_t num_classes, bool aux)\\n{\\n    int64_t in_channels = 2048; // resnet\\uC758 4\\uBC88\\uC9F8 \\uB808\\uC774\\uC5B4\\uC758 \\uCD9C\\uB825 \\uAC12\\uC774 2048 * 60 * 60 \\uC774\\uAE30\\uB54C\\uBB38\\uC5D0 \\uC785\\uB825\\uC740 2048 \\uC785\\uB2C8\\uB2E4.\\n\\n    /*\\n        _classifier \\uC2E4\\uC81C\\uB85C Segmentation\\uC744 \\uC9C4\\uD589 \\uD558\\uB294 \\uBAA8\\uB378 \\uC785\\uB2C8\\uB2E4. \\n        \\uD574\\uB2F9 \\uBAA8\\uB378\\uC740 \\uB17C\\uBB38\\uC744 \\uCC38\\uACE0 \\uD558\\uC2DC\\uB294\\uAC8C \\uB3C4\\uC6C0\\uC774 \\uB418\\uC2E4\\uAEBC\\uC5D0\\uC694. \\uC5B4\\uC90D\\uC9E2\\uC740 \\uC9C0\\uC2DD\\uC758 \\uC804\\uB2EC \\uBCF4\\uB2E4 \\uC0AC\\uC6A9 \\uBC95\\uC5D0 \\uC9D1\\uC911 \\uD560\\uAC8C\\uC694.\\n        \\uCF54\\uB4DC\\uB294 \\uC544\\uB798\\uC640 \\uAC19\\uC774 \\uC120\\uC5B8 \\uD569\\uB2C8\\uB2E4.\\n    */\\n    _classifier = torch::nn::Sequential\\n    (\\n        ASPP(ASPPImpl(2048, { 12,24,36 })), \\n        torch::nn::Conv2d(\\n            torch::nn::Conv2dOptions(256, 256, 3).padding(1).with_bias(false)),\\n        torch::nn::BatchNorm(\\n            torch::nn::BatchNormOptions(256).eps(0.001).momentum(0.01)),\\n        torch::nn::Functional(torch::relu),\\n        torch::nn::Conv2d(\\n            torch::nn::Conv2dOptions(256, num_classes, 1))\\n    );\\n\\n    /*\\n     aux\\uB780 resnet\\uC758 3\\uBC88\\uC9F8 \\uB808\\uC774\\uC5B4\\uC758 \\uCD9C\\uB825 \\uAC12\\uC744 Segmentation\\uC5D0 \\uD65C\\uC6A9 \\uD560\\uC9C0 \\uC5EC\\uBD80\\uC778\\uB370\\uC694. \\uC800\\uAC19\\uC740 \\uACBD\\uC6B0\\uC5D0\\uB294 \\uC0AC\\uC6A9\\uD558\\uC9C0 \\uC54A\\uC558\\uC2B5\\uB2C8\\uB2E4.\\n    */\\n    \\n    if (aux != false)\\n    {\\n        _aux = aux;\\n        _aux_classifier = _make_FCNHead(1024, num_classes);\\n    }\\n\\n    // \\uC544\\uB798\\uB294 \\uC704\\uC5D0\\uC11C \\uC124\\uBA85 \\uB4DC\\uB838\\uB124\\uC694.\\n    ResNet101 Resnet;   \\n    torch::load(Resnet, \\\"resnet101_Python.pt\\\");\\n\\n    _backbone = IntermediateLayerGetter(IntermediateLayerGetterImpl(std::move(Resnet), {\\\"layer3\\\",\\\"layer4\\\"}));\\n\\n    register_module(\\\"backbone\\\", _backbone);\\n    register_module(\\\"classifier\\\", _classifier);\\n    register_module(\\\"aux_classifier\\\", _aux_classifier);\\n}\\n\\n/*\\n SegmentationModelImpl::forward\\uB294 \\uCD9C\\uB825\\uC778 2\\uAC1C \\uC77C \\uC218\\uB3C4 \\uC788\\uACE0 1\\uAC1C \\uC77C \\uC218\\uB3C4 \\uC788\\uC2B5\\uB2C8\\uB2E4.\\n std::unordered_map<std::string, torch::Tensor> \\uD615\\uD0DC\\uB85C \\uCD9C\\uB825 \\uB418\\uACE0 Key \\uAC12\\uC744 out\\uACFC aux \\uB9AC\\uD134 \\uD569\\uB2C8\\uB2E4.\\n \\n \\uC774\\uBBF8\\uC9C0\\uB97C _backbone\\uC5D0 forward \\uD6C4 \\uCD9C\\uB825 \\uAC12\\uC744 _classifier forward \\uD6C4 \\uCD9C\\uB825 \\uB41C \\uAC12\\uC744 \\n \\uC774\\uBBF8\\uC9C0 \\uC0AC\\uC774\\uC988 \\uB9CC\\uD07C \\uB2E4\\uC2DC upsample \\uD558\\uC5EC \\uCD9C\\uB825 \\uD569\\uB2C8\\uB2E4. aux\\uB97C \\uC0AC\\uC6A9 \\uD560 \\uACBD\\uC6B0\\uC5D0 _aux_classifier \\uCD9C\\uB825 \\uAC12\\uB3C4 \\uB9AC\\uD134 \\uD569\\uB2C8\\uB2E4.\\n\\n*/\\nstd::unordered_map<std::string, torch::Tensor> SegmentationModelImpl::forward(torch::Tensor x)\\n{\\n    std::unordered_map<std::string, torch::Tensor> result;\\n\\n    int64_t h = x.size(2), w = x.size(3);\\n\\n    auto feature = _backbone->forward(x);\\n\\n    x = feature[1];\\n\\n    x = _classifier->forward(x);\\n    x = torch::upsample_bilinear2d(x, { h,w }, false);\\n    result.insert(std::make_pair(\\\"out\\\", x));\\n\\n    if (_aux == true)\\n    {\\n        x = feature[0];\\n        x = _aux_classifier->forward(x);\\n        x = torch::upsample_bilinear2d(x, { h,w }, false);\\n        result.insert(std::make_pair(\\\"aux\\\", x));\\n    }\\n\\n    return result;\\n}  \\n\\n\")), mdx(\"h1\", {\n    \"id\": \"convert-python-model-parameter-to-c\"\n  }, \"Convert Python Model Parameter to C++\"), mdx(\"p\", null, \"Convert Python Model \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://kerry-cho.github.io/TransferLearning-Libtorch/\"\n  }, \"TransferLearning\"), \" \\uD3EC\\uC2A4\\uD2B8\\uB97C \\uCC38\\uACE0 \\uD558\\uC2DC\\uBA74 \\uB429\\uB2C8\\uB2E4.\"), mdx(\"h1\", {\n    \"id\": \"dataset\"\n  }, \"DataSet\"), mdx(\"p\", null, \"\\uB370\\uC774\\uD130 \\uC14B\\uC740 MS COCO \\uB370\\uC774\\uD130 \\uC14B\\uC744 \\uC0AC\\uC6A9 \\uD558\\uC600\\uC2B5\\uB2C8\\uB2E4.\\n\\uB370\\uC774\\uD130 \\uC14B\\uC740 \\uD074\\uB798\\uC2A4\\uB294 \\uBA54\\uC778 \\uAC1D\\uCCB4\\uC640 2\\uAC1C\\uC758 \\uD558\\uC704 \\uAC1D\\uCCB4\\uB97C \\uD3EC\\uD568 \\uD558\\uACE0 \\uC788\\uC2B5\\uB2C8\\uB2E4. \\uC2DC\\uAC01\\uD654\\uB97C \\uC704\\uD574 UML\\uC744 \\uADF8\\uB824 \\uBCF4\\uC558\\uC2B5\\uB2C8\\uB2E4.\\nCOCODataSet \\uD074\\uB798\\uC2A4\\uC5D0\\uC11C \\uBA64\\uBC84 \\uD074\\uB798\\uC2A4\\uB85C CocoDetection \\uD074\\uB798\\uC2A4\\uB97C \\uAC00\\uC9C0\\uACE0  CocoDetection\\uC5D0\\uC11C CocoData\\uC14B\\uC758 JSON Parser\\uC778 CocoNote\\uB97C \\uD3EC\\uD568 \\uD558\\uACE0 \\uC788\\uC2B5\\uB2C8\\uB2E4.\"), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"872px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"34.03508771929824%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABB0lEQVQY002QbYuFIBCF+/8/L25FhBJ1syJfUvOqqONlk93t+TAwh3PmwFSMsa7rmqbp+14IoZQCAMbYMAyv16tt22mapJTee6XUuq7FqZRijFXTNPU327ZZa8/zBABCyDiOnHMhhDFGSmmtJYRgjOu6nufZe7+uazUMw/v9zjkDQIxRa51zxhgTQlJK4eY8T+89QqjrulIbQtj3vUIIGWOcc9ZaY8xxHM45jPGyLDHGEAIAcM6llMuyIITGcQwh5JwppRWlFADyTUqpXL2uq4hlaq2NMZ/PJ6UUYyziTzg/SCmVmLX2qV/X5W7K+h/WWqtfhBDlYZRS9YBz7r1njP2Ztdbbtn0BzTCK5+GpMbYAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"dataset\",\n    \"title\": \"dataset\",\n    \"src\": \"/static/0ab5fe90c2d7995c9a8bed564a2aff7d/bdb44/dataset.png\",\n    \"srcSet\": [\"/static/0ab5fe90c2d7995c9a8bed564a2aff7d/050f2/dataset.png 285w\", \"/static/0ab5fe90c2d7995c9a8bed564a2aff7d/2b0be/dataset.png 570w\", \"/static/0ab5fe90c2d7995c9a8bed564a2aff7d/bdb44/dataset.png 872w\"],\n    \"sizes\": \"(max-width: 872px) 100vw, 872px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  }), \"\\n    \")), mdx(\"p\", null, \"COCODataSet \\uAD6C\\uC870  \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-c++\"\n  }, \"/*\\nCOCODataSet\\uC740 \\uAC1D\\uCCB4 \\uC0DD\\uC131\\uC2DC 4\\uAC1C\\uC758 \\uC785\\uB825\\uC744 \\uBC1B\\uC2B5\\uB2C8\\uB2E4.\\nannFile  = annotation File\\uC758 \\uC804\\uCCB4 \\uACBD\\uB85C\\nroot     = \\uC774\\uBBF8\\uC9C0 \\uD30C\\uC77C\\uC774 \\uC788\\uB294 \\uACBD\\uB85C ,annotation\\uC5D0\\uB294 \\uC774\\uBBF8\\uC9C0 \\uD30C\\uC77C \\uBA85\\uB9CC \\uD45C\\uC2DC \\uB418\\uC5B4 \\uC788\\uC2B5\\uB2C8\\uB2E4.\\ncat_list = \\uD559\\uC2B5 \\uC2DC\\uD0A4\\uAE38 \\uC6D0\\uD558\\uB294 \\uCE74\\uD14C\\uACE0\\uB9AC \\uC815\\uBCF4 0\\uC740 Background \\uC774\\uBA70 , \\uC22B\\uC790\\uB85C \\uC785\\uB825\\uB429\\uB2C8\\uB2E4\\n           \\uD574\\uB2F9 \\uC22B\\uC790\\uB294 MS COCO 2017 \\uCE74\\uD14C\\uACE0\\uB9AC \\uC815\\uBCF4\\uB97C \\uCC38\\uACE0 \\uD558\\uC138\\uC694.\\nremove_images_without_annotations = \\uC8FC\\uC11D\\uC774 \\uC5C6\\uB294 \\uC774\\uBBF8\\uC9C0\\uC758 \\uC0AD\\uC81C \\uC5EC\\uBD80\\n\\nExample\\nauto val_dataset = COCODataSet(data_dir + \\\"annotations\\\\\\\\instances_val2017.json\\\", data_dir + \\\"val2017\\\", true, { 0,17,18 })\\n    .map(torch::data::transforms::Stack<>());\\nconst size_t va_dataset_size = val_dataset.size().value();\\n\\n*/\\n\\n\\nclass COCODataSet : public torch::data::Dataset<COCODataSet>\\n{\\nprivate:\\n\\n    std::vector<torch::Tensor> states, labels;\\n    size_t ds_size;\\n    torch::data::transforms::Normalize<> normalizeChannels;\\npublic:\\n    COCODataSet(std::string annFile, std::string root, bool remove_images_without_annotations \\n        , std::vector<int> cat_list = std::vector<int>{});\\n\\n    torch::data::Example<> get(size_t index) override;\\n    torch::optional<size_t> size() const override;\\n\\n    rcnn::data::COCODetection _coco_detection;\\n    std::vector<int> _cat_list;\\n    std::map<int, int> _cat_idx;\\n};\\n// \\uD558\\uC704 \\uAC1D\\uCCB4 \\uC124\\uBA85 \\uD6C4\\uC5D0 \\uB354 \\uC790\\uC138\\uD788 \\uC124\\uBA85 \\uD558\\uACA0\\uC2B5\\uB2C8\\uB2E4.\\n\")), mdx(\"p\", null, \"COCODetection\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-c++\"\n  }, \"\\n/*\\nCOCODetection \\uC758 \\uACBD\\uC6B0 \\uC0DD\\uC131\\uC2DC 2\\uAC1C\\uC758 \\uC785\\uB825\\uC744 \\uBC1B\\uC2B5\\uB2C8\\uB2E4.\\nannFile  = annotation File\\uC758 \\uC804\\uCCB4 \\uACBD\\uB85C\\nroot     = \\uC774\\uBBF8\\uC9C0 \\uD30C\\uC77C\\uC774 \\uC788\\uB294 \\uACBD\\uB85C ,annotation\\uC5D0\\uB294 \\uC774\\uBBF8\\uC9C0 \\uD30C\\uC77C \\uBA85\\uB9CC \\uD45C\\uC2DC \\uB418\\uC5B4 \\uC788\\uC2B5\\uB2C8\\uB2E4.\\n*/\\nnamespace rcnn {\\nnamespace data \\n{\\n\\nclass COCODetection : public torch::data::datasets::Dataset<COCODetection, torch::data::Example<cv::Mat, std::vector<Annotation>>> \\n{\\n\\npublic:\\n    COCODetection(std::string root, std::string annFile/*TODO transform=*/);\\n    torch::data::Example<cv::Mat, std::vector<Annotation>> get(size_t index) override;\\n    torch::optional<size_t> size() const override;\\n\\n    std::string _root;\\n    COCONote _coco;\\n    std::vector<int> _ids;\\n\\n    friend std::ostream& operator << (std::ostream& os, const COCODetection& bml);\\n};\\n\\n}//data\\n}//rcnn\\n\\n/*\\n\\uC774\\uBBF8\\uC9C0\\uD30C\\uC77C\\uC758 \\uC778\\uB371\\uC2A4 \\uC815\\uBCF4\\uB97C \\uAC00\\uC838\\uC640 \\uD574\\uB2F9 \\uC774\\uBBF8\\uC9C0\\uC758 Annotation \\uC815\\uBCF4\\uC640 \\uC774\\uBBF8\\uC9C0\\uD30C\\uC77C\\uC744 \\uB85C\\uB529 \\uD558\\uC5EC \\uBC18\\uD658 \\uD569\\uB2C8\\uB2E4.\\n*/\\ntorch::data::Example<cv::Mat, std::vector<Annotation>> COCODetection::get(size_t index) \\n{\\n    int img_id = _ids.at(index);\\n    std::vector<int64_t> ann_ids = _coco.GetAnnIds(std::vector<int>{img_id}); //Image ID\\n    std::vector<Annotation> target = _coco.LoadAnns(ann_ids); // Load Anotations\\n    std::string path(_coco.LoadImgs(std::vector<int>{img_id})[0]._file_name); //LoadImage\\n    cv::Mat img = cv::imread(_root + \\\"/\\\" + path, cv::IMREAD_COLOR);\\n\\n    if (img.rows == 0)\\n    {\\n        std::cout << \\\"The image does not exist.\\\" << std::endl;\\n        std::cout << _root + \\\"/\\\" + path << std::endl;\\n        quick_exit(1);\\n    }\\n\\n\\n    torch::data::Example<cv::Mat, std::vector<Annotation>> value{ img, target };\\n    return value;\\n}\\n\\n/*\\n\\uC804\\uCCB4 \\uB370\\uC774\\uD130 \\uC0AC\\uC774\\uC988\\uB97C \\uBC18\\uD658\\n*/\\ntorch::optional<size_t> COCODetection::size() const\\n{\\n    return _ids.size();\\n}\\n\")), mdx(\"p\", null, \"COCONote\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-c++\"\n  }, \"\\n/*\\nPoco::JSON\\uC758 JSON\\uC744 \\uC0AC\\uC6A9\\uD574 annotation \\uC815\\uBCF4\\uB97C \\uC77D\\uC5B4 \\uB4DC\\uB9AC\\uB294 \\uD074\\uB798\\uC2A4 \\uC785\\uB2C8\\uB2E4.\\nPoco\\uB294 \\uD574\\uB2F9 \\uD504\\uB85C\\uC81D\\uD2B8\\uB97C \\uC704\\uD574 \\uC9C1\\uC811 \\uBE4C\\uB4DC \\uD558\\uC5EC \\uC0AC\\uC6A9 \\uD558\\uC600\\uC2B5\\uB2C8\\uB2E4.\\n\\uAE30\\uD68C\\uAC00 \\uB41C\\uB2E4\\uBA74 \\uC790\\uC138\\uD788 \\uB9AC\\uBDF0\\uB97C \\uD558\\uB294 \\uD3EC\\uC2A4\\uD305\\uC744 \\uC9C4\\uD589 \\uD558\\uACA0\\uC2B5\\uB2C8\\uB2E4.\\n*/\\nstruct COCONote\\n{\\n    COCONote(std::string annotation_file);\\n    COCONote();\\n\\n    void Parse();\\n    void Parse(std::string annotation_file);\\n\\n    std::vector<int64_t> GetAnnIds(const std::vector<int> imgIds = std::vector<int>{}, const std::vector<int> catIds = std::vector<int>{}, const std::vector<float> areaRng = \\n...\\n};\\n\\n/*\\n\\uC9C1\\uC811 Parsing\\uC744 \\uC9C4\\uD589 \\uD558\\uB294 \\uD568\\uC218 \\uC785\\uB2C8\\uB2E4.\\n*/\\nvoid COCONote::Parse()\\n{\\n#ifdef _DEBUG\\n    std::cout << \\\"Parse...\\\\n\\\";\\n#endif\\n    if(_cocodataset->has(\\\"annotations\\\"))//annotations \\uC815\\uBCF4\\uAC00 \\uC788\\uB294\\uC9C0 \\uD655\\uC778 \\uD569\\uB2C8\\uB2E4.\\n    {\\n        assert(_cocodataset->get(\\\"annotations\\\").isArray()); //annotations\\uC774 Array\\uAC1D\\uCCB4\\uAC00 \\uC544\\uB2D0 \\uACBD\\uC6B0 \\uC608\\uC678\\uB97C \\uBC1C\\uC0DD \\uC2DC\\uD0B5\\uB2C8\\uB2E4.\\n\\n        Array::Ptr a = _cocodataset->get(\\\"annotations\\\").extract<Array::Ptr>(); //annotations \\uC744 ArryPtr \\uD0C0\\uC785\\uC73C\\uB85C \\uBCC0\\uD658 \\uD569\\uB2C8\\uB2E4.\\n\\n        for(int i = 0; i < a->size(); i++)//Array Size \\uB9CC\\uD07C for\\uB97C \\uC9C4\\uD589 \\uD569\\uB2C8\\uB2E4.\\n        {\\n            Object::Ptr j = a->get(i).extract<Object::Ptr>(); //\\uAC01 \\uC778\\uB371\\uC2A4 \\uBCC4\\uB85C Object::Ptr\\uBCC0\\uD658 \\uD569\\uB2C8\\uB2E4.\\n\\n            if(_imgToAnns.count(j->get(\\\"image_id\\\").convert<int>()))//\\uC774\\uBBF8\\uC9C0\\uC758 id\\uB97C \\uAC00\\uC838\\uC640 _imgToAnns\\uC5D0 \\uC788\\uB294\\uC9C0 \\uBE44\\uAD50\\uD569\\uB2C8\\uB2E4.\\n            { // if it exists\\n                _imgToAnns[j->get(\\\"image_id\\\").convert<int>()].emplace_back(j);// \\uC874\\uC7AC \\uD560\\uACBD\\uC6B0\\uC5D0 \\uD574\\uB2F9 Key\\uAC12\\uC5D0 \\uC9D1\\uC5B4 \\uB123\\uC2B5\\uB2C8\\uB2E4.\\n            }\\n            else\\n            {\\n                _imgToAnns[j->get(\\\"image_id\\\").convert<int>()] = std::vector<Annotation> {Annotation(j)};// \\uC5C6\\uC744 \\uACBD\\uC6B0\\uC5D0\\uB294 \\uC0C8\\uB85C\\uC6B4 Annotation\\uB9CC\\uB4E4\\uC5B4 _imgToAnns\\uC5D0 \\uB2F4\\uC2B5\\uB2C8\\uB2E4.\\n            }\\n            \\n            _anns[static_cast<int64_t>(j->get(\\\"id\\\").convert<int64_t>())] = Annotation(j);\\n        }\\n    }\\n\\n    ....\\n}\\n\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Data Arguments\", mdx(\"br\", {\n    parentName: \"li\"\n  }), \"\\uC774\\uBBF8\\uC9C0\\uC640 Annotation \\uC815\\uBCF4\\uB294 COCODataSet Get \\uD568\\uC218\\uAC00 \\uD638\\uCD9C \\uB420\\uC2DC\\uC5D0 \\uC9C4\\uD589 \\uB429\\uB2C8\\uB2E4.\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-c++\"\n  }, \"\\n/*\\n\\uD568\\uC218\\uAC00 \\uC0C1\\uB2F9\\uD788 \\uAE38\\uC9C0\\uB9CC \\uC0C1\\uC138\\uD558\\uAC8C \\uC124\\uBA85 \\uD558\\uACE0 \\uC8FC\\uC758 \\uAE4A\\uAC8C \\uBCF4\\uB294\\uAC8C \\uC88B\\uC744 \\uAC83 \\uAC19\\uC2B5\\uB2C8\\uB2E4.\\n*/\\ntorch::data::Example<> COCODataSet::get(size_t idx)\\n{\\n    auto coco_data = _coco_detection.get(idx);\\n    cv::Mat img = coco_data.data;\\n    \\n    //\\uD604\\uC7AC idx Annotation \\uC815\\uBCF4\\uB97C \\uAC00\\uC838\\uC640 \\uB0B4\\uAC00 \\uD559\\uC2B5 \\uD558\\uACE0\\uC790 \\uD558\\uB294 \\uCE74\\uD14C\\uACE0\\uB9AC\\uB97C \\uC81C\\uC678\\uD55C \\uC815\\uBCF4\\uB294 \\uC0AD\\uC81C \\uD569\\uB2C8\\uB2E4.\\n\\n    std::vector<Annotation> anno = coco_data.target;\\n    for (auto ann = anno.begin(); ann != anno.end();) \\n    {\\n        if (std::find(_cat_list.begin(), _cat_list.end(), ann->_category_id) == _cat_list.end())\\n        {\\n            anno.erase(ann);\\n        }\\n        else\\n        {\\n            ann++;\\n        }\\n    }\\n\\n    //Annotation \\uC815\\uBCF4\\uB294 Polygon \\uD615\\uD0DC\\uC758 \\uC790\\uB8CC\\uD615\\uC73C\\uB85C \\uB418\\uC5B4 \\uC788\\uC2B5\\uB2C8\\uB2E4. \\uD574\\uB2F9 \\uC815\\uBCF4\\uB97C \\n    // H * W \\uD615\\uD0DC\\uC758 Matrix \\uD0C0\\uC785\\uC758 \\uAD6C\\uC870\\uB85C \\uD0C0\\uC785 \\uBCC0\\uD658\\uC744 \\uC9C4\\uD589 \\uD558\\uB294 \\uD568\\uC218 \\uC785\\uB2C8\\uB2E4.\\n    // Matrix \\uC815\\uBCF4\\uC5D0\\uC11C \\uD574\\uB2F9 \\uCE74\\uD14C\\uACE0\\uB9AC \\uC601\\uC5ED\\uC740 \\uAC12\\uC774 1\\uC774\\uACE0 , \\uB098\\uBA38\\uC9C0\\uB294 0\\uC758 \\uAC12\\uC774 \\uCC44\\uC6CC \\uC9D1\\uB2C8\\uB2E4.\\n    // COCO DataSet\\uC758 Polygon \\uC815\\uBCF4\\uB294  x1,y1,x2,y2,x3,y3,xn,yn\\uC758 \\uD615\\uD0DC\\uB85C double Array \\uD0C0\\uC785\\uC73C\\uB85C \\uB418\\uC5B4 \\uC788\\uC2B5\\uB2C8\\uB2E4.\\n    std::vector<int> cats;\\n    std::vector<std::vector<std::vector<double>>> polys;\\n    for (auto& obj : anno)\\n    {\\n        polys.push_back(obj._segmentation);//Annotation _segmentation\\uC815\\uBCF4\\uB9CC \\uAC00\\uC838 \\uC635\\uB2C8\\uB2E4. PolyLines\\n        cats.push_back(_cat_idx[obj._category_id]);//\\uC704 Polygon\\uC758 Category ID\\uC5D0 \\uD574\\uB2F9 \\uD558\\uB294 \\uC778\\uB371\\uC2A4\\uB97C \\uAC00\\uC838 \\uC635\\uB2C8\\uB2E4.\\n    //\\uC608\\uC81C\\uC5D0\\uC11C\\uB294 Category list\\uAC00 {0 , 17 ,18} \\uB85C \\uC785\\uB825 \\uB418\\uC5C8\\uC2B5\\uB2C8\\uB2E4.\\n    //\\uC608\\uB97C \\uB4E4\\uC5B4 \\uACE0\\uC591\\uC774 \\uC77C \\uACBD\\uC6B0 Category ID\\uAC00 17\\uC774\\uAE30 \\uB54C\\uBB38\\uC5D0 index\\uB294 1\\uC774 \\uC785\\uB825 \\uB429\\uB2C8\\uB2E4.\\n    }\\n\\n    std::vector<torch::Tensor>  mask_tensors;\\n\\n    //\\uC774\\uBBF8\\uC9C0\\uC640 \\uB9C8\\uC2A4\\uD06C\\uC758 \\uC785\\uB825 \\uC0AC\\uC774\\uC988\\uB294 480\\uC73C\\uB85C \\uBCC0\\uACBD \\uD558\\uAE30\\uC704\\uD55C Base Size \\uC785\\uB2C8\\uB2E4.\\n    int base_size = 480;\\n\\n    //Polygon To Mask Tensors\\n    for (int k= 0; k< polys.size(); k++)\\n    {\\n        //Polygon\\uC758 \\uC0AC\\uC774\\uC988\\uAC00 0 \\uC77C \\uACBD\\uC6B0 \\uB9AC\\uD134 \\uD569\\uB2C8\\uB2E4.\\n        if (polys[k].size() == 0) continue;\\n        \\n        //\\uD604\\uC7AC \\uB85C\\uB529 \\uB41C \\uC774\\uBBF8\\uC9C0\\uC640 Base\\uC0AC\\uC774\\uC9C0\\uB97C \\uBE44\\uAD50\\uD574 scale \\uAC12\\uC744 \\uAD6C\\uD55C \\uD6C4 Polygon\\uC744 Resize \\uD574\\uC90D\\uB2C8\\uB2E4.     \\n        transforms::polygon::Resize((double)base_size / (double)img.cols, (double)base_size / (double)img.rows, polys[k]);\\n\\n\\n        //coco API\\uB97C \\uC0AC\\uC6A9 \\uD558\\uAE30\\uC704\\uD574\\uC11C Polygon \\uC815\\uBCF4\\uB97C coco API \\uC5D0\\uC11C \\uC0AC\\uC6A9\\uD558\\uB294 \\uC790\\uB8CC \\uD615\\uC73C\\uB85C \\uBCC0\\uD658 \\uD6C4 \\n        //Mask \\uC815\\uBCF4\\uB97C \\uB9AC\\uD134 \\uBC1B\\uC2B5\\uB2C8\\uB2E4.\\n        auto frPoly = coco::frPoly(polys[k], base_size, base_size);\\n\\n        coco::RLEs Rs(1);\\n\\n        coco::rleFrString(Rs._R, (char*)frPoly[0].counts.c_str(), std::get<0>(frPoly[0].size), std::get<1>(frPoly[0].size));\\n        coco::siz h = Rs._R[0].h, w = Rs._R[0].w, n = Rs._n;\\n        coco::Masks masks = coco::Masks(base_size, base_size, 1);\\n\\n        coco::rleDecode(Rs._R, masks._mask, n);\\n\\n        //coco API \\uBCC0\\uD658\\uD55C Mask Size \\uB9CC\\uD07C \\uBE44\\uC5B4 \\uC788\\uB294 Tensor \\uC0DD\\uC131 \\uD569\\uB2C8\\uB2E4.\\n        int shape = h * w * n;\\n        torch::Tensor mask_tensor = torch::empty({ shape });\\n\\n        float* data1 = mask_tensor.data_ptr<float>(); //\\uD574\\uB2F9 Tensor\\uB97C Dataptr \\uBCC0\\uACBD \\uD6C4\\uC5D0 \\n        for (size_t i = 0; i < shape; ++i) {\\n            data1[i] = static_cast<float>(masks._mask[i] * cats[k]);//Category Index\\uB97C \\uACF1\\uD55C \\uD6C4 \\uAC12\\uC744 \\uBCF5\\uC0AC \\uD574 \\uC90D\\uB2C8\\uB2E4.\\n        }\\n\\n        //Mask\\uB294 \\uD574\\uB2F9 Category \\uC601\\uC5ED\\uC740 1, \\uC544\\uB2D0 \\uACBD\\uC6B0 0 \\uC774\\uAE30 \\uB54C\\uBB38\\uC5D0 Category ID \\uACF1\\uD558\\uBA74 \\uAC1C \\uC77C \\uACBD\\uC6B0 2\\uB85C \\uBCC0\\uACBD \\uB418\\uACE0\\n        //\\uC544\\uB2CC \\uC601\\uC5ED\\uC740 0\\uC73C\\uB85C \\uCC44\\uC6CC \\uC9D1\\uB2C8\\uB2E4.\\n\\n        //Mask_tensor\\uB97C Category\\uC5D0 Mapping \\uD6C4\\uC5D0 \\uC774\\uBBF8\\uC9C0 \\uC0AC\\uC774\\uC640 \\uB3D9\\uC77C\\uD55C Matrix \\uD615\\uD0DC\\uB85C \\uBCC0\\uACBD \\uD569\\uB2C8\\uB2E4\\n        // h * w \\n        mask_tensor = mask_tensor.reshape({ static_cast<int64_t>(n),static_cast<int64_t>(w),\\n            static_cast<int64_t>(h) }).permute({ 2, 1, 0 }).squeeze(2);//fortran order h, w, n\\n\\n        mask_tensors.push_back(mask_tensor);\\n    }\\n    \\n    // mask_tensors\\uB294 Vector\\uB97C Tensor Type\\uB85C \\uBCC0\\uACBD \\uD569\\uB2C8\\uB2E4.\\n    // n * h * w \\uD615\\uD0DC\\uB85C \\uBCC0\\uD658 \\uB429\\uB2C8\\uB2E4.\\n    auto mask_tensor = torch::stack(mask_tensors); \\n\\n    //n * h * w \\uD615\\uD0DC\\uC758 Tensor\\uB97C \\uD558\\uB098\\uB85C \\uD569\\uCE58\\uAC8C \\uB429\\uB2C8\\uB2E4.\\n    //\\uC608 4 * h * w -> h * w;\\n    // \\uB3D9\\uC77C\\uD55C \\uC601\\uC5ED\\uC758 \\uAC12\\uC744 \\uCDE8\\uD558\\uB294 \\uAC8C \\uC544\\uB2C8\\uB77C Max \\uAC12\\uB9CC \\uAC00\\uC838 \\uC635\\uB2C8\\uB2E4.\\n    // \\uC989 \\uC11C\\uB85C \\uB2E4\\uB978 \\uCE74\\uD14C\\uACE0\\uB9AC \\uB9C8\\uC2A4\\uD06C\\uB97C \\uD558\\uB098\\uB85C \\uD569\\uCE69\\uB2C8\\uB2E4.\\n    torch::Tensor target, _;\\n    std::tie(target, _) = torch::max(mask_tensor, 0);\\n    \\n    //\\uD604\\uC7AC \\uB85C\\uB529 \\uB41C \\uC774\\uBBF8\\uC9C0\\uB97C Resizng \\uD569\\uB2C8\\uB2E4.\\n    cv::resize(img, img, cv::Size(base_size, base_size));\\n\\n\\n    //Random \\uAC12\\uC774 2\\uC758 \\uBC30\\uC218\\uC77C \\uACBD\\uC6B0 \\uC774\\uBBF8\\uC9C0\\uC640 target\\uC744 Horizental Flip\\uC744 \\uC9C4\\uD589 \\uD569\\uB2C8\\uB2E4.\\n    if (die(mersenne) % 2 == 0)\\n    {\\n        target = target.flip({ 1 });\\n        cv::flip(img, img, 1);\\n    }\\n\\n    torch::Tensor img_tensor = torch::from_blob(img.data, { img.rows, img.cols, 3 }, torch::kByte);\\n    img_tensor = img_tensor.permute({ 2, 0, 1 });\\n\\n    //\\uC774\\uBBF8\\uC9C0\\uB97C normalize \\uD569\\uB2C8\\uB2E4.\\n    img_tensor = normalizeChannels(img_tensor);\\n\\n    // \\uC544\\uB798 \\uCF54\\uB4DC\\uB294 \\uC774\\uBBF8\\uC640 Tensor\\uAC00 \\uC81C\\uB300\\uB85C \\uC785\\uB825\\uC774 \\uB418\\uB294\\uC9C0 \\uD655\\uC778 \\uD558\\uAE30 \\uC704\\uD55C Debug  \\uCF54\\uB4DC \\uC785\\uB2C8\\uB2E4.\\n#if 0 // Debug Data Inputs\\n    std::cout << img_tensor.sizes() << std::endl;\\n    std::cout << target.sizes() << std::endl;\\n\\n    cv::Mat bin_mask = cv::Mat::eye(target.size(0), target.size(1), CV_8UC1);\\n    target = target.clamp(0, 255).to(torch::kU8);\\n    target = target.to(torch::kCPU);\\n    std::memcpy(bin_mask.data, target.data_ptr(), sizeof(torch::kU8) * target.numel());\\n\\n    uchar* data_ptr = (uchar*)bin_mask.data;\\n\\n    for (int y = 0; y < bin_mask.rows; y++)\\n    {\\n        for (int x = 0; x < bin_mask.cols; x++)\\n        {\\n            if (data_ptr[y * bin_mask.cols + x] == 0)\\n            {\\n                continue;\\n            }\\n            else\\n            {\\n                data_ptr[y * bin_mask.cols + x]  = 255;\\n            }\\n        }\\n    }\\n\\n    cv::imshow(\\\"Image\\\", bin_mask);\\n    cv::imshow(\\\"Image2\\\", img);\\n    cv::waitKey(0);\\n#endif\\n\\n    //Image\\uC640 Tensor\\uB97C Tuple \\uD615\\uD0DC\\uB85C \\uBC18\\uD658 \\uD569\\uB2C8\\uB2E4.\\n    return { img_tensor.clone(), target.clone() };\\n}\\n\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Data Loader  & Opimizer  \")), mdx(\"p\", null, \"Data Loader Opimizer\\uC758 \\uC0C1\\uC138\\uD55C \\uC815\\uBCF4\\uB294 \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://kerry-cho.github.io/TransferLearning-Libtorch/\"\n  }, \"TransferLearning\"), \" \\uD3EC\\uC2A4\\uD2B8\\uB97C \\uCC38\\uACE0 \\uD558\\uC2DC\\uBA74 \\uB429\\uB2C8\\uB2E4.\\n\\uB2E4\\uB978 \\uC810\\uB9CC \\uC124\\uBA85 \\uD558\\uACA0\\uC2B5\\uB2C8\\uB2E4.  \"), mdx(\"p\", null, \"Segmentation\\uC5D0\\uC11C\\uB294 Backbone\\uC774 \\uC874\\uC7AC\\uD558\\uACE0 \\uD574\\uB2F9 \\uBAA8\\uB378\\uC758 \\uACBD\\uC6B0 Aux \\uB77C\\uB294 \\uC635\\uC158\\uC774 \\uC788\\uAE30 \\uB54C\\uBB38\\uC5D0 \\uADF8\\uC5D0 \\uB530\\uB77C Optimizer\\uC5D0\\uC11C \\uD559\\uC2B5 \\uD574\\uC57C \\uD558\\uB294\", mdx(\"br\", {\n    parentName: \"p\"\n  }), \"\\n\", \"\\uD559\\uC2B5 \\uD30C\\uB77C\\uBA54\\uD130\\uAC00 \\uC870\\uAE08 \\uC529 \\uB2E4\\uB985\\uB2C8\\uB2E4.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-c++\"\n  }, \"\\n/*\\nTransferLearning \\uD06C\\uAC8C \\uB2E4\\uB978 \\uC810\\uC740 Backbone \\uD30C\\uB77C\\uBA54\\uD130\\uC640 _classifier\\uC758 \\uD30C\\uB77C\\uBA54\\uD130 \\naux_classifier\\uD30C\\uB77C\\uBA54\\uD130\\uB97C \\uAC01\\uAC01 std::vector<torch::Tensor> \\uB2F4\\uC544 \\uC8FC\\uC5B4\\uC57C \\uD55C\\uB2E4\\uB294 \\uC810\\uC785\\uB2C8\\uB2E4.\\n\\uD574\\uB2F9 \\uC608\\uC81C\\uB294 aux_classifier\\uB97C \\uC0AC\\uC6A9 \\uD558\\uC9C0 \\uC54A\\uAE30 \\uB54C\\uBB38\\uC5D0 \\uC0DD\\uB7B5 \\uD558\\uC600\\uC2B5\\uB2C8\\uB2E4.\\n*/\\nstd::vector<torch::Tensor> trainable_params;    \\nauto params = segnet->_classifier->named_parameters(true /*recurse*/);\\nfor (auto& param : params)\\n{\\n    auto layer_name = param.key();\\n\\n    if (param.value().requires_grad())\\n    {\\n        trainable_params.push_back(param.value());\\n    }\\n}\\n\\nparams = segnet->_backbone->named_parameters(true /*recurse*/);\\nfor (auto& param : params)\\n{\\n    if (param.value().requires_grad())\\n    {\\n        trainable_params.push_back(param.value());\\n    }\\n}\\n\\ntorch::optim::SGD optimizer(trainable_params, torch::optim::SGDOptions(0.01 /*learning rate*/).momentum(0.9).weight_decay(1e-4));\\n\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Training Loop & test Loop\")), mdx(\"p\", null, \"\\uD559\\uC2B5\\uACFC \\uD14C\\uC2A4\\uD2B8\\uB294 TransferLearning \\uD06C\\uAC8C \\uB2E4\\uB974\\uC9C0 \\uC54A\\uC2B5\\uB2C8\\uB2E4. \\uB530\\uB77C\\uC11C Loss\\uB97C \\uAD6C\\uD558\\uB294 \\uD568\\uC218\\uC5D0 \\uB300\\uD574\\uC11C\\uB9CC \\uC124\\uBA85 \\uB4DC\\uB9AC\\uACE0 \\uB098\\uBA38\\uC9C0 \\uBD80\\uBD84\\uC740\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://kerry-cho.github.io/TransferLearning-Libtorch/\"\n  }, \"TransferLearning\"), \" \\uD3EC\\uC2A4\\uD2B8\\uB97C \\uCC38\\uACE0 \\uD558\\uC2DC\\uBA74 \\uB429\\uB2C8\\uB2E4.\"), mdx(\"p\", null, \"loss\\uD568\\uC218\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-c++\"\n  }, \"\\n/*\\nloss \\uD568\\uC218\\uB294 Classcification\\uCC98\\uB7FC CrossEntropy\\uB97C \\uAD6C\\uD558\\uB294\\uB370\\uC694. Segmentation\\uC758 \\uACBD\\uC6B0 Pixel \\uC804\\uCCB4\\uB97C \\uBE44\\uAD50 \\uD55C\\uB2E4\\uACE0 \\uC0DD\\uAC01 \\uD558\\uC2DC\\uBA74 \\uB429\\uB2C8\\uB2E4.\\n\\uD574\\uB2F9 \\uD568\\uC218\\uB294 LibTorch\\uC5D0 \\uC874\\uC7AC \\uD558\\uC9C0 \\uC54A\\uC544\\uC11C Network \\uCD9C\\uB825 \\uAC12\\uC744 log_softmax \\uCDE8\\uD55C \\uD6C4 nll_loss2d \\uD568\\uC218\\uB85C \\uACC4\\uC0B0 \\uD569\\uB2C8\\uB2E4.\\n*/\\ntorch::Tensor criterion(\\n    std::unordered_map<std::string, torch::Tensor> inputs, torch::Tensor target)\\n{\\n    std::map<std::string, torch::Tensor> losses;\\n\\n    for (auto loss : inputs)\\n    {\\n        losses[loss.first] = torch::nll_loss2d(torch::log_softmax(loss.second, 1), target, {}, 1, 255);\\n    }\\n\\n    if (losses.size() == 1)\\n    {\\n        return losses[\\\"out\\\"];\\n    }\\n\\n    return losses[\\\"out\\\"] + 0.5 * losses[\\\"aux\\\"];\\n}\\n\")), mdx(\"h2\", {\n    \"id\": \"conclusion\"\n  }, \"Conclusion\"), mdx(\"p\", null, \"\\uC9C0\\uAE08\\uAE4C\\uC9C0 LibTorch\\uB97C \\uC774\\uC6A9\\uD55C Segmentation \\uB300\\uD574\\uC11C \\uC774\\uC57C\\uAE30\\uB97C \\uD558\\uC600\\uC2B5\\uB2C8\\uB2E4. \\uC5EC\\uAE30 \\uAE4C\\uC9C0 \\uC77D\\uC5B4 \\uC8FC\\uC2E0\\uAC83\\uC744 \\uAC10\\uC0AC \\uB4DC\\uB9BD\\uB2C8\\uB2E4.\\nSegmentation\\uC5D0 \\uB300\\uD574 \\uC774\\uB860\\uC801\\uC73C\\uB85C \\uC811\\uADFC \\uD558\\uAE30 \\uBCF4\\uB2E4 LibTorch\\uB97C \\uD65C\\uC6A9 \\uD55C \\uBC29\\uBC95\\uC5D0 \\uB300\\uD574\\uC11C \\uC124\\uBA85\\uC744 \\uC9D1\\uC911 \\uD558\\uC600\\uC2B5\\uB2C8\\uB2E4. \\uD3EC\\uC2A4\\uD305 \\uCD08\\uBC18\\uC5D0 Segmentation\\uC774\\uB77C\\uB294\\n\\uC0AC\\uC804\\uC9C0\\uC2DD\\uC774 \\uD544\\uC694 \\uD558\\uB2E4\\uACE0 \\uD588\\uB294\\uB370 \\uC2E4\\uC81C \\uD574\\uB2F9 \\uB0B4\\uC6A9\\uC5D0 \\uB300\\uD55C \\uC124\\uBA85\\uC740 \\uC9C4\\uD589 \\uD558\\uC9C0 \\uC54A\\uC740 \\uAC83 \\uAC19\\uB124\\uC694. \\uD558\\uC9C0\\uB9CC Segmentation\\uC774 \\uBB34\\uC5C7\\uC744 \\uD558\\uB294 \\uAC83\\uC774\\uB780 \\uAC78\\n\\uC54C\\uACE0 \\uD574\\uB2F9 \\uD3EC\\uC2A4\\uD2B8\\uB97C \\uC77D\\uB294 \\uAC83\\uC774 \\uC911\\uC694 \\uD558\\uB2E4\\uACE0 \\uC0DD\\uAC01\\uD558\\uC600\\uC2B5\\uB2C8\\uB2E4.\\n\\uAD81\\uAE08 \\uD55C \\uB0B4\\uC6A9\\uC774\\uB098, \\uC798\\uBABB \\uB41C \\uC810\\uC774 \\uC788\\uB2E4\\uBA74 \\uB2F5\\uAE00\\uC5D0 \\uB0A8\\uACA8 \\uC8FC\\uC2DC\\uBA74 \\uAC10\\uC0AC \\uD558\\uACA0\\uC2B5\\uB2C8\\uB2E4.\\n\\uB2E4\\uC74C \\uC774\\uC57C\\uAE30\\uB294 Microsoft\\uC5D0\\uC11C \\uC9C4\\uD589 \\uD558\\uB294 OpenSource \\uD504\\uB85C\\uC81D\\uD2B8\\uC911 VoTT\\uC5D0 \\uB300\\uD55C \\uB9AC\\uBDF0\\uB97C \\uC9C4\\uD589 \\uD558\\uACA0\\uC2B5\\uB2C8\\uB2E4(Visual Object Tagging Tool)\\n\\uAC10\\uC0AC\\uD569\\uB2C8\\uB2E4.\"));\n}\n;\nMDXContent.isMDXComponent = true;","keywords":null,"tags":[{"id":"5092ad6b-ccaa-590e-bf86-709fa148732c","name":"#Libtorch","slug":"/tag/libtorch/"},{"id":"fd2a0ba3-b929-5b2a-94cf-a16d000dff5d","name":"#Pytorh","slug":"/tag/pytorh/"}],"thumbnail":{"__typename":"ImageSharp","ImageSharp_vertical":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/ae32ed553a4f4e890f82b95f87388ee3/c3c3a/image.jpg","srcSet":"/static/ae32ed553a4f4e890f82b95f87388ee3/25bbe/image.jpg 190w,\n/static/ae32ed553a4f4e890f82b95f87388ee3/c3c3a/image.jpg 380w","sizes":"(min-width: 380px) 380px, 100vw"},"sources":[{"srcSet":"/static/ae32ed553a4f4e890f82b95f87388ee3/c50be/image.webp 190w,\n/static/ae32ed553a4f4e890f82b95f87388ee3/c4b86/image.webp 380w","type":"image/webp","sizes":"(min-width: 380px) 380px, 100vw"}]},"width":380,"height":290},"ImageSharp_hero":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/ae32ed553a4f4e890f82b95f87388ee3/b104e/image.jpg","srcSet":"/static/ae32ed553a4f4e890f82b95f87388ee3/2c034/image.jpg 800w,\n/static/ae32ed553a4f4e890f82b95f87388ee3/b104e/image.jpg 1600w","sizes":"(min-width: 1600px) 1600px, 100vw"},"sources":[{"srcSet":"/static/ae32ed553a4f4e890f82b95f87388ee3/dd444/image.webp 800w,\n/static/ae32ed553a4f4e890f82b95f87388ee3/68ede/image.webp 1600w","type":"image/webp","sizes":"(min-width: 1600px) 1600px, 100vw"}]},"width":1600,"height":650}}},"tagCategoryPosts":{"nodes":[{"id":"007d0b0a-5333-5915-a199-fe6c6085c0fd","title":"TransferLearning Libtorch","slug":"/transferlearning-libtorch/","link":null,"excerpt":"안녕하세요. 조대희 입니다.\n블로그 방문을 환영 합니다. 첫 번째로 소개 해드릴 내용은 PyTorch의 C++ Frontend 인 LibTorch…","timeToRead":2,"featured":null,"thumbnailText":null,"date":"November 27, 2019","category":{"id":"9551985a-0ac8-5537-8160-577b93e7bd47","name":"deeplearing","slug":"/category/deeplearing/","color":null,"icon":"/static/9db8414257e39f04182ba00bd97007bb/deeplearing.svg"},"author":{"id":"6ade7dba-bac0-5d88-95e2-844c9469c477","name":"Kerry Cho","slug":"/author/kerry-cho/","title":"Software Engineer","description":"","skills":["C++","C","Node"],"social":[{"name":"Instagram","url":"https://instagram.com/instagram"},{"name":"Twitter","url":"https://twitter.com/twitter"},{"name":"Website","url":"https://example.com"}],"thumbnail":{"__typename":"ImageSharp","ImageSharp_small":{"layout":"fixed","placeholder":{"fallback":"data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='150'%20height='150'%20viewBox='0%200%20150%20150'%20preserveAspectRatio='none'%3e%3cpath%20d='M60%206c-6%204-10%209-11%2018-2%207-1%209%204%2024l3%209c0%201-1%202-3%202l-5%204c-3%205-7%208-10%207l-2%202c0%203-2%204-2%201-1-2-1%205-1%2010l-1%203-1-1c0-2-3-5-4-4l1%207c1%205%201%206-1%2010l-2%205-1%202c-2%202-2%208%200%209%201%201%200%205-1%205s-2%201-2%203l1%203v4c-2%201-1%202%203%201h5c3%201%202%208-2%209l-4%202-2%202%203%201%204%204c2%202%203%202%2043%202h40l2-3c2-2%202-4%202-6v-4l2-4c1-3%201-3-3-3-5-1-6-1-4%201%203%203%201%203-4%200l-5-4v-1l-6-1-5%201%203-2%204-2h-6c-2%201-3%202%200-5%202-5%201-5-2%200-2%203-4%203-4%201l1-1%201-1v-3l-2-2c-1-2-1-2-4-1-2%202-2%202-3%200l-2-2c-1%200-2-3-1-4l1%201h2c0-1%202-1%204%201l4%202c1%200-2-5-4-6-1-2-1-2%203-6%203-4%204-5%203-10a2391%202391%200%2001-1-8h-2v-9l1-8-3-3c-4-3-4-7-1-10s7-23%206-29C87%208%2071%201%2060%206m-6%2062l-4%201c-3%200-6%205-6%208v3c-1%201%200%201%202%201l4%201h4c5-2%2012%200%2011%204-1%203-9%204-9%201l4-1c3%200%203%200%202-1H44c-1%201%200%201%201%201%204%200%204%201%202%203h-4c-2-2-3%200-2%204%201%205%203%206%2011%206s12-1%2014-6c2-3%203-9%201-9l-2-5c0-4-1-5-4-8-5-4-5-4-7-3m50%2012l-4%208-5%2010c-1%203-3%207-2%208l-2%202c-2%201-2%201%201%202%202%201%204%202%206%207%203%207%206%209%2010%209l2%201c2%202%206%202%208-1%203-4%203-4-1-3l-4-1-7-2c-5%200-5-1-5-3-1-1%200-2%204-3l7-6%204-5c1-1%203-14%201-13l-17%208c-1%200%200-3%204-6%203-5%204-8%201-4-4%203-4%202%200-5%203-6%203-8-1-3'%20fill='%23d3d3d3'%20fill-rule='evenodd'/%3e%3c/svg%3e"},"images":{"fallback":{"src":"/static/852c30a1756768fa576889f312b2bd64/a7a66/kerry-cho.png","srcSet":"/static/852c30a1756768fa576889f312b2bd64/a7a66/kerry-cho.png 48w,\n/static/852c30a1756768fa576889f312b2bd64/416a0/kerry-cho.png 96w","sizes":"48px"},"sources":[{"srcSet":"/static/852c30a1756768fa576889f312b2bd64/a3542/kerry-cho.webp 48w,\n/static/852c30a1756768fa576889f312b2bd64/0f66d/kerry-cho.webp 96w","type":"image/webp","sizes":"48px"}]},"width":48,"height":48},"ImageSharp_regular":{"layout":"fixed","placeholder":{"fallback":"data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='150'%20height='150'%20viewBox='0%200%20150%20150'%20preserveAspectRatio='none'%3e%3cpath%20d='M60%206c-6%204-10%209-11%2018-2%207-1%209%204%2024l3%209c0%201-1%202-3%202l-5%204c-3%205-7%208-10%207l-2%202c0%203-2%204-2%201-1-2-1%205-1%2010l-1%203-1-1c0-2-3-5-4-4l1%207c1%205%201%206-1%2010l-2%205-1%202c-2%202-2%208%200%209%201%201%200%205-1%205s-2%201-2%203l1%203v4c-2%201-1%202%203%201h5c3%201%202%208-2%209l-4%202-2%202%203%201%204%204c2%202%203%202%2043%202h40l2-3c2-2%202-4%202-6v-4l2-4c1-3%201-3-3-3-5-1-6-1-4%201%203%203%201%203-4%200l-5-4v-1l-6-1-5%201%203-2%204-2h-6c-2%201-3%202%200-5%202-5%201-5-2%200-2%203-4%203-4%201l1-1%201-1v-3l-2-2c-1-2-1-2-4-1-2%202-2%202-3%200l-2-2c-1%200-2-3-1-4l1%201h2c0-1%202-1%204%201l4%202c1%200-2-5-4-6-1-2-1-2%203-6%203-4%204-5%203-10a2391%202391%200%2001-1-8h-2v-9l1-8-3-3c-4-3-4-7-1-10s7-23%206-29C87%208%2071%201%2060%206m-6%2062l-4%201c-3%200-6%205-6%208v3c-1%201%200%201%202%201l4%201h4c5-2%2012%200%2011%204-1%203-9%204-9%201l4-1c3%200%203%200%202-1H44c-1%201%200%201%201%201%204%200%204%201%202%203h-4c-2-2-3%200-2%204%201%205%203%206%2011%206s12-1%2014-6c2-3%203-9%201-9l-2-5c0-4-1-5-4-8-5-4-5-4-7-3m50%2012l-4%208-5%2010c-1%203-3%207-2%208l-2%202c-2%201-2%201%201%202%202%201%204%202%206%207%203%207%206%209%2010%209l2%201c2%202%206%202%208-1%203-4%203-4-1-3l-4-1-7-2c-5%200-5-1-5-3-1-1%200-2%204-3l7-6%204-5c1-1%203-14%201-13l-17%208c-1%200%200-3%204-6%203-5%204-8%201-4-4%203-4%202%200-5%203-6%203-8-1-3'%20fill='%23d3d3d3'%20fill-rule='evenodd'/%3e%3c/svg%3e"},"images":{"fallback":{"src":"/static/852c30a1756768fa576889f312b2bd64/46e60/kerry-cho.png","srcSet":"/static/852c30a1756768fa576889f312b2bd64/46e60/kerry-cho.png 150w,\n/static/852c30a1756768fa576889f312b2bd64/d612b/kerry-cho.png 300w","sizes":"150px"},"sources":[{"srcSet":"/static/852c30a1756768fa576889f312b2bd64/ae23d/kerry-cho.webp 150w,\n/static/852c30a1756768fa576889f312b2bd64/bd37b/kerry-cho.webp 300w","type":"image/webp","sizes":"150px"}]},"width":150,"height":150}}},"thumbnail":{"__typename":"ImageSharp","ImageSharp_vertical":{"layout":"constrained","backgroundColor":"#080818","images":{"fallback":{"src":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/c3c3a/image.jpg","srcSet":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/25bbe/image.jpg 190w,\n/static/ddd2714304cbf2da6a3c5bd43c9cc061/c3c3a/image.jpg 380w","sizes":"(min-width: 380px) 380px, 100vw"},"sources":[{"srcSet":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/c50be/image.webp 190w,\n/static/ddd2714304cbf2da6a3c5bd43c9cc061/c4b86/image.webp 380w","type":"image/webp","sizes":"(min-width: 380px) 380px, 100vw"}]},"width":380,"height":290},"ImageSharp_hero":{"layout":"constrained","backgroundColor":"#080818","images":{"fallback":{"src":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/b104e/image.jpg","srcSet":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/2c034/image.jpg 800w,\n/static/ddd2714304cbf2da6a3c5bd43c9cc061/b104e/image.jpg 1600w","sizes":"(min-width: 1600px) 1600px, 100vw"},"sources":[{"srcSet":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/dd444/image.webp 800w,\n/static/ddd2714304cbf2da6a3c5bd43c9cc061/68ede/image.webp 1600w","type":"image/webp","sizes":"(min-width: 1600px) 1600px, 100vw"}]},"width":1600,"height":650}}}]},"tagPosts":{"nodes":[{"id":"007d0b0a-5333-5915-a199-fe6c6085c0fd","title":"TransferLearning Libtorch","slug":"/transferlearning-libtorch/","link":null,"excerpt":"안녕하세요. 조대희 입니다.\n블로그 방문을 환영 합니다. 첫 번째로 소개 해드릴 내용은 PyTorch의 C++ Frontend 인 LibTorch…","timeToRead":2,"featured":null,"thumbnailText":null,"date":"November 27, 2019","category":{"id":"9551985a-0ac8-5537-8160-577b93e7bd47","name":"deeplearing","slug":"/category/deeplearing/","color":null,"icon":"/static/9db8414257e39f04182ba00bd97007bb/deeplearing.svg"},"author":{"id":"6ade7dba-bac0-5d88-95e2-844c9469c477","name":"Kerry Cho","slug":"/author/kerry-cho/","title":"Software Engineer","description":"","skills":["C++","C","Node"],"social":[{"name":"Instagram","url":"https://instagram.com/instagram"},{"name":"Twitter","url":"https://twitter.com/twitter"},{"name":"Website","url":"https://example.com"}],"thumbnail":{"__typename":"ImageSharp","ImageSharp_small":{"layout":"fixed","placeholder":{"fallback":"data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='150'%20height='150'%20viewBox='0%200%20150%20150'%20preserveAspectRatio='none'%3e%3cpath%20d='M60%206c-6%204-10%209-11%2018-2%207-1%209%204%2024l3%209c0%201-1%202-3%202l-5%204c-3%205-7%208-10%207l-2%202c0%203-2%204-2%201-1-2-1%205-1%2010l-1%203-1-1c0-2-3-5-4-4l1%207c1%205%201%206-1%2010l-2%205-1%202c-2%202-2%208%200%209%201%201%200%205-1%205s-2%201-2%203l1%203v4c-2%201-1%202%203%201h5c3%201%202%208-2%209l-4%202-2%202%203%201%204%204c2%202%203%202%2043%202h40l2-3c2-2%202-4%202-6v-4l2-4c1-3%201-3-3-3-5-1-6-1-4%201%203%203%201%203-4%200l-5-4v-1l-6-1-5%201%203-2%204-2h-6c-2%201-3%202%200-5%202-5%201-5-2%200-2%203-4%203-4%201l1-1%201-1v-3l-2-2c-1-2-1-2-4-1-2%202-2%202-3%200l-2-2c-1%200-2-3-1-4l1%201h2c0-1%202-1%204%201l4%202c1%200-2-5-4-6-1-2-1-2%203-6%203-4%204-5%203-10a2391%202391%200%2001-1-8h-2v-9l1-8-3-3c-4-3-4-7-1-10s7-23%206-29C87%208%2071%201%2060%206m-6%2062l-4%201c-3%200-6%205-6%208v3c-1%201%200%201%202%201l4%201h4c5-2%2012%200%2011%204-1%203-9%204-9%201l4-1c3%200%203%200%202-1H44c-1%201%200%201%201%201%204%200%204%201%202%203h-4c-2-2-3%200-2%204%201%205%203%206%2011%206s12-1%2014-6c2-3%203-9%201-9l-2-5c0-4-1-5-4-8-5-4-5-4-7-3m50%2012l-4%208-5%2010c-1%203-3%207-2%208l-2%202c-2%201-2%201%201%202%202%201%204%202%206%207%203%207%206%209%2010%209l2%201c2%202%206%202%208-1%203-4%203-4-1-3l-4-1-7-2c-5%200-5-1-5-3-1-1%200-2%204-3l7-6%204-5c1-1%203-14%201-13l-17%208c-1%200%200-3%204-6%203-5%204-8%201-4-4%203-4%202%200-5%203-6%203-8-1-3'%20fill='%23d3d3d3'%20fill-rule='evenodd'/%3e%3c/svg%3e"},"images":{"fallback":{"src":"/static/852c30a1756768fa576889f312b2bd64/a7a66/kerry-cho.png","srcSet":"/static/852c30a1756768fa576889f312b2bd64/a7a66/kerry-cho.png 48w,\n/static/852c30a1756768fa576889f312b2bd64/416a0/kerry-cho.png 96w","sizes":"48px"},"sources":[{"srcSet":"/static/852c30a1756768fa576889f312b2bd64/a3542/kerry-cho.webp 48w,\n/static/852c30a1756768fa576889f312b2bd64/0f66d/kerry-cho.webp 96w","type":"image/webp","sizes":"48px"}]},"width":48,"height":48},"ImageSharp_regular":{"layout":"fixed","placeholder":{"fallback":"data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='150'%20height='150'%20viewBox='0%200%20150%20150'%20preserveAspectRatio='none'%3e%3cpath%20d='M60%206c-6%204-10%209-11%2018-2%207-1%209%204%2024l3%209c0%201-1%202-3%202l-5%204c-3%205-7%208-10%207l-2%202c0%203-2%204-2%201-1-2-1%205-1%2010l-1%203-1-1c0-2-3-5-4-4l1%207c1%205%201%206-1%2010l-2%205-1%202c-2%202-2%208%200%209%201%201%200%205-1%205s-2%201-2%203l1%203v4c-2%201-1%202%203%201h5c3%201%202%208-2%209l-4%202-2%202%203%201%204%204c2%202%203%202%2043%202h40l2-3c2-2%202-4%202-6v-4l2-4c1-3%201-3-3-3-5-1-6-1-4%201%203%203%201%203-4%200l-5-4v-1l-6-1-5%201%203-2%204-2h-6c-2%201-3%202%200-5%202-5%201-5-2%200-2%203-4%203-4%201l1-1%201-1v-3l-2-2c-1-2-1-2-4-1-2%202-2%202-3%200l-2-2c-1%200-2-3-1-4l1%201h2c0-1%202-1%204%201l4%202c1%200-2-5-4-6-1-2-1-2%203-6%203-4%204-5%203-10a2391%202391%200%2001-1-8h-2v-9l1-8-3-3c-4-3-4-7-1-10s7-23%206-29C87%208%2071%201%2060%206m-6%2062l-4%201c-3%200-6%205-6%208v3c-1%201%200%201%202%201l4%201h4c5-2%2012%200%2011%204-1%203-9%204-9%201l4-1c3%200%203%200%202-1H44c-1%201%200%201%201%201%204%200%204%201%202%203h-4c-2-2-3%200-2%204%201%205%203%206%2011%206s12-1%2014-6c2-3%203-9%201-9l-2-5c0-4-1-5-4-8-5-4-5-4-7-3m50%2012l-4%208-5%2010c-1%203-3%207-2%208l-2%202c-2%201-2%201%201%202%202%201%204%202%206%207%203%207%206%209%2010%209l2%201c2%202%206%202%208-1%203-4%203-4-1-3l-4-1-7-2c-5%200-5-1-5-3-1-1%200-2%204-3l7-6%204-5c1-1%203-14%201-13l-17%208c-1%200%200-3%204-6%203-5%204-8%201-4-4%203-4%202%200-5%203-6%203-8-1-3'%20fill='%23d3d3d3'%20fill-rule='evenodd'/%3e%3c/svg%3e"},"images":{"fallback":{"src":"/static/852c30a1756768fa576889f312b2bd64/46e60/kerry-cho.png","srcSet":"/static/852c30a1756768fa576889f312b2bd64/46e60/kerry-cho.png 150w,\n/static/852c30a1756768fa576889f312b2bd64/d612b/kerry-cho.png 300w","sizes":"150px"},"sources":[{"srcSet":"/static/852c30a1756768fa576889f312b2bd64/ae23d/kerry-cho.webp 150w,\n/static/852c30a1756768fa576889f312b2bd64/bd37b/kerry-cho.webp 300w","type":"image/webp","sizes":"150px"}]},"width":150,"height":150}}},"thumbnail":{"__typename":"ImageSharp","ImageSharp_vertical":{"layout":"constrained","backgroundColor":"#080818","images":{"fallback":{"src":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/c3c3a/image.jpg","srcSet":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/25bbe/image.jpg 190w,\n/static/ddd2714304cbf2da6a3c5bd43c9cc061/c3c3a/image.jpg 380w","sizes":"(min-width: 380px) 380px, 100vw"},"sources":[{"srcSet":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/c50be/image.webp 190w,\n/static/ddd2714304cbf2da6a3c5bd43c9cc061/c4b86/image.webp 380w","type":"image/webp","sizes":"(min-width: 380px) 380px, 100vw"}]},"width":380,"height":290},"ImageSharp_hero":{"layout":"constrained","backgroundColor":"#080818","images":{"fallback":{"src":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/b104e/image.jpg","srcSet":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/2c034/image.jpg 800w,\n/static/ddd2714304cbf2da6a3c5bd43c9cc061/b104e/image.jpg 1600w","sizes":"(min-width: 1600px) 1600px, 100vw"},"sources":[{"srcSet":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/dd444/image.webp 800w,\n/static/ddd2714304cbf2da6a3c5bd43c9cc061/68ede/image.webp 1600w","type":"image/webp","sizes":"(min-width: 1600px) 1600px, 100vw"}]},"width":1600,"height":650}}}]},"categoryPosts":{"nodes":[{"id":"007d0b0a-5333-5915-a199-fe6c6085c0fd","title":"TransferLearning Libtorch","slug":"/transferlearning-libtorch/","link":null,"excerpt":"안녕하세요. 조대희 입니다.\n블로그 방문을 환영 합니다. 첫 번째로 소개 해드릴 내용은 PyTorch의 C++ Frontend 인 LibTorch…","timeToRead":2,"featured":null,"thumbnailText":null,"date":"November 27, 2019","category":{"id":"9551985a-0ac8-5537-8160-577b93e7bd47","name":"deeplearing","slug":"/category/deeplearing/","color":null,"icon":"/static/9db8414257e39f04182ba00bd97007bb/deeplearing.svg"},"author":{"id":"6ade7dba-bac0-5d88-95e2-844c9469c477","name":"Kerry Cho","slug":"/author/kerry-cho/","title":"Software Engineer","description":"","skills":["C++","C","Node"],"social":[{"name":"Instagram","url":"https://instagram.com/instagram"},{"name":"Twitter","url":"https://twitter.com/twitter"},{"name":"Website","url":"https://example.com"}],"thumbnail":{"__typename":"ImageSharp","ImageSharp_small":{"layout":"fixed","placeholder":{"fallback":"data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='150'%20height='150'%20viewBox='0%200%20150%20150'%20preserveAspectRatio='none'%3e%3cpath%20d='M60%206c-6%204-10%209-11%2018-2%207-1%209%204%2024l3%209c0%201-1%202-3%202l-5%204c-3%205-7%208-10%207l-2%202c0%203-2%204-2%201-1-2-1%205-1%2010l-1%203-1-1c0-2-3-5-4-4l1%207c1%205%201%206-1%2010l-2%205-1%202c-2%202-2%208%200%209%201%201%200%205-1%205s-2%201-2%203l1%203v4c-2%201-1%202%203%201h5c3%201%202%208-2%209l-4%202-2%202%203%201%204%204c2%202%203%202%2043%202h40l2-3c2-2%202-4%202-6v-4l2-4c1-3%201-3-3-3-5-1-6-1-4%201%203%203%201%203-4%200l-5-4v-1l-6-1-5%201%203-2%204-2h-6c-2%201-3%202%200-5%202-5%201-5-2%200-2%203-4%203-4%201l1-1%201-1v-3l-2-2c-1-2-1-2-4-1-2%202-2%202-3%200l-2-2c-1%200-2-3-1-4l1%201h2c0-1%202-1%204%201l4%202c1%200-2-5-4-6-1-2-1-2%203-6%203-4%204-5%203-10a2391%202391%200%2001-1-8h-2v-9l1-8-3-3c-4-3-4-7-1-10s7-23%206-29C87%208%2071%201%2060%206m-6%2062l-4%201c-3%200-6%205-6%208v3c-1%201%200%201%202%201l4%201h4c5-2%2012%200%2011%204-1%203-9%204-9%201l4-1c3%200%203%200%202-1H44c-1%201%200%201%201%201%204%200%204%201%202%203h-4c-2-2-3%200-2%204%201%205%203%206%2011%206s12-1%2014-6c2-3%203-9%201-9l-2-5c0-4-1-5-4-8-5-4-5-4-7-3m50%2012l-4%208-5%2010c-1%203-3%207-2%208l-2%202c-2%201-2%201%201%202%202%201%204%202%206%207%203%207%206%209%2010%209l2%201c2%202%206%202%208-1%203-4%203-4-1-3l-4-1-7-2c-5%200-5-1-5-3-1-1%200-2%204-3l7-6%204-5c1-1%203-14%201-13l-17%208c-1%200%200-3%204-6%203-5%204-8%201-4-4%203-4%202%200-5%203-6%203-8-1-3'%20fill='%23d3d3d3'%20fill-rule='evenodd'/%3e%3c/svg%3e"},"images":{"fallback":{"src":"/static/852c30a1756768fa576889f312b2bd64/a7a66/kerry-cho.png","srcSet":"/static/852c30a1756768fa576889f312b2bd64/a7a66/kerry-cho.png 48w,\n/static/852c30a1756768fa576889f312b2bd64/416a0/kerry-cho.png 96w","sizes":"48px"},"sources":[{"srcSet":"/static/852c30a1756768fa576889f312b2bd64/a3542/kerry-cho.webp 48w,\n/static/852c30a1756768fa576889f312b2bd64/0f66d/kerry-cho.webp 96w","type":"image/webp","sizes":"48px"}]},"width":48,"height":48},"ImageSharp_regular":{"layout":"fixed","placeholder":{"fallback":"data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='150'%20height='150'%20viewBox='0%200%20150%20150'%20preserveAspectRatio='none'%3e%3cpath%20d='M60%206c-6%204-10%209-11%2018-2%207-1%209%204%2024l3%209c0%201-1%202-3%202l-5%204c-3%205-7%208-10%207l-2%202c0%203-2%204-2%201-1-2-1%205-1%2010l-1%203-1-1c0-2-3-5-4-4l1%207c1%205%201%206-1%2010l-2%205-1%202c-2%202-2%208%200%209%201%201%200%205-1%205s-2%201-2%203l1%203v4c-2%201-1%202%203%201h5c3%201%202%208-2%209l-4%202-2%202%203%201%204%204c2%202%203%202%2043%202h40l2-3c2-2%202-4%202-6v-4l2-4c1-3%201-3-3-3-5-1-6-1-4%201%203%203%201%203-4%200l-5-4v-1l-6-1-5%201%203-2%204-2h-6c-2%201-3%202%200-5%202-5%201-5-2%200-2%203-4%203-4%201l1-1%201-1v-3l-2-2c-1-2-1-2-4-1-2%202-2%202-3%200l-2-2c-1%200-2-3-1-4l1%201h2c0-1%202-1%204%201l4%202c1%200-2-5-4-6-1-2-1-2%203-6%203-4%204-5%203-10a2391%202391%200%2001-1-8h-2v-9l1-8-3-3c-4-3-4-7-1-10s7-23%206-29C87%208%2071%201%2060%206m-6%2062l-4%201c-3%200-6%205-6%208v3c-1%201%200%201%202%201l4%201h4c5-2%2012%200%2011%204-1%203-9%204-9%201l4-1c3%200%203%200%202-1H44c-1%201%200%201%201%201%204%200%204%201%202%203h-4c-2-2-3%200-2%204%201%205%203%206%2011%206s12-1%2014-6c2-3%203-9%201-9l-2-5c0-4-1-5-4-8-5-4-5-4-7-3m50%2012l-4%208-5%2010c-1%203-3%207-2%208l-2%202c-2%201-2%201%201%202%202%201%204%202%206%207%203%207%206%209%2010%209l2%201c2%202%206%202%208-1%203-4%203-4-1-3l-4-1-7-2c-5%200-5-1-5-3-1-1%200-2%204-3l7-6%204-5c1-1%203-14%201-13l-17%208c-1%200%200-3%204-6%203-5%204-8%201-4-4%203-4%202%200-5%203-6%203-8-1-3'%20fill='%23d3d3d3'%20fill-rule='evenodd'/%3e%3c/svg%3e"},"images":{"fallback":{"src":"/static/852c30a1756768fa576889f312b2bd64/46e60/kerry-cho.png","srcSet":"/static/852c30a1756768fa576889f312b2bd64/46e60/kerry-cho.png 150w,\n/static/852c30a1756768fa576889f312b2bd64/d612b/kerry-cho.png 300w","sizes":"150px"},"sources":[{"srcSet":"/static/852c30a1756768fa576889f312b2bd64/ae23d/kerry-cho.webp 150w,\n/static/852c30a1756768fa576889f312b2bd64/bd37b/kerry-cho.webp 300w","type":"image/webp","sizes":"150px"}]},"width":150,"height":150}}},"thumbnail":{"__typename":"ImageSharp","ImageSharp_vertical":{"layout":"constrained","backgroundColor":"#080818","images":{"fallback":{"src":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/c3c3a/image.jpg","srcSet":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/25bbe/image.jpg 190w,\n/static/ddd2714304cbf2da6a3c5bd43c9cc061/c3c3a/image.jpg 380w","sizes":"(min-width: 380px) 380px, 100vw"},"sources":[{"srcSet":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/c50be/image.webp 190w,\n/static/ddd2714304cbf2da6a3c5bd43c9cc061/c4b86/image.webp 380w","type":"image/webp","sizes":"(min-width: 380px) 380px, 100vw"}]},"width":380,"height":290},"ImageSharp_hero":{"layout":"constrained","backgroundColor":"#080818","images":{"fallback":{"src":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/b104e/image.jpg","srcSet":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/2c034/image.jpg 800w,\n/static/ddd2714304cbf2da6a3c5bd43c9cc061/b104e/image.jpg 1600w","sizes":"(min-width: 1600px) 1600px, 100vw"},"sources":[{"srcSet":"/static/ddd2714304cbf2da6a3c5bd43c9cc061/dd444/image.webp 800w,\n/static/ddd2714304cbf2da6a3c5bd43c9cc061/68ede/image.webp 1600w","type":"image/webp","sizes":"(min-width: 1600px) 1600px, 100vw"}]},"width":1600,"height":650}}}]},"previous":{"id":"007d0b0a-5333-5915-a199-fe6c6085c0fd","slug":"/transferlearning-libtorch/","title":"TransferLearning Libtorch"},"next":{"id":"a835629e-5e8f-5974-a212-2ca9c047221c","slug":"/custom-git-bash/","title":"Custom Git Bash"}},"pageContext":{"id":"f62d8d44-100c-54f9-9904-98cc9554fc11","categoryId":"9551985a-0ac8-5537-8160-577b93e7bd47","tagsIds":["5092ad6b-ccaa-590e-bf86-709fa148732c","fd2a0ba3-b929-5b2a-94cf-a16d000dff5d"],"hasTags":true,"previousId":"007d0b0a-5333-5915-a199-fe6c6085c0fd","nextId":"a835629e-5e8f-5974-a212-2ca9c047221c","paginatePostsPage":true,"basePath":"/","services":{"algolia":false,"mailchimp":false,"disqus":true},"siteUrl":null,"mobileMenu":{"title":"Topics","items":[{"name":"deeplearing","slug":"/category/deeplearing/"},{"name":"others","slug":"/category/others/"}]},"darkMode":true}},"staticQueryHashes":["1434057858","1992822086","2918496967","4235339838","4240507859"]}